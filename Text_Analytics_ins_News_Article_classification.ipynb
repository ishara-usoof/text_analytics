{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "I8fFhhgdxDnu"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "G_MhBr2TxDnv"
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "3HJtJbXBEce_",
    "outputId": "bafff579-27d7-48ca-f75a-5e6f30289db9"
   },
   "outputs": [],
   "source": [
    "# Data Loading\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "path = r'C:\\Users\\TA\\TextAnalysis_News'\n",
    "\n",
    "ada_derena_news = pd.read_csv(os.path.join(path , 'ada_derena_news.csv'))\n",
    "ada_derana_sports_news = pd.read_csv(os.path.join(path , 'ada_derana_sports_news.csv'))\n",
    "ceylon_today_news = pd.read_csv(os.path.join(path , 'ceylon_today_news.csv'))\n",
    "colombo_gazette_news = pd.read_csv(os.path.join(path, 'colombo_gazette_news.csv'))\n",
    "daily_mirror_news = pd.read_csv(os.path.join(path , 'daily_mirror_news.csv'))\n",
    "lanka_express_news = pd.read_csv(os.path.join(path , 'lanka_express_news.csv'))\n",
    "news_1st = pd.read_csv(os.path.join(path , 'news_1st.csv'))\n",
    "news_wire = pd.read_csv(os.path.join(path, 'news_wire.csv'))\n",
    "sri_lanka_mirror_news = pd.read_csv(os.path.join(path, 'sri_lanka_mirror_news.csv'))\n",
    "the_morning_news = pd.read_csv(os.path.join(path, 'the_morning_news.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NM5gz8f23zzP"
   },
   "outputs": [],
   "source": [
    "lanka_news_sources = [ada_derena_news, ada_derana_sports_news, ceylon_today_news, colombo_gazette_news, daily_mirror_news, lanka_express_news, news_1st, news_wire, sri_lanka_mirror_news, the_morning_news]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "goHOEyjb4PJE"
   },
   "outputs": [],
   "source": [
    "# Function to rename column names\n",
    "def rename_columns(dataframes):\n",
    "    for source_df in dataframes:\n",
    "        # Rename the first column to 'articles'\n",
    "        source_df.rename(columns={source_df.columns[0]: 'Articles'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s1TXYInR4Y7B"
   },
   "outputs": [],
   "source": [
    "# Call the function to rename columns\n",
    "rename_columns(lanka_news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SW6SKLfABG0"
   },
   "outputs": [],
   "source": [
    "source_names = [\"Ada Derena News\", \"Ada Derana Sports News\", \"Ceylon Today News\", \"Colombo Gazette News\", \"Daily Mirror News\", \"Lanka Express News\", \"News 1st\", \"News Wire\", \"Sri Lanka Mirror News\", \"The Morning News\"]\n",
    "def add_source_name_column(dataframes, source_names):\n",
    "    for source_df, source_name in zip(dataframes, source_names):\n",
    "        source_df.insert(0, 'Source_Name', source_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ooCFI5tmAmIX"
   },
   "outputs": [],
   "source": [
    "add_source_name_column(lanka_news_sources, source_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aAr_TgiSHP01",
    "outputId": "8acdb62b-0ea1-4405-b5bc-c9eed9fa12b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[         Source_Name                                           Articles\n",
      "0    Ada Derena News  (2, 'CBSL decides not to renew permits of 15 m...\n",
      "1    Ada Derena News  (2, 'X-Press Pearl disaster: SC calls for repo...\n",
      "2    Ada Derena News  (2, 'Sri Lanka News | Breaking News in Sri Lan...\n",
      "3    Ada Derena News  (2, 'Torrential rain, floods kill 22 across no...\n",
      "4    Ada Derena News  (2, 'Driver arrested over bus accident in Mana...\n",
      "..               ...                                                ...\n",
      "775  Ada Derena News  (2, 'සුදුවෙන්න ගිය අයගේ ඇත්ත තත්ත්වය මෙන්න.. \"...\n",
      "776  Ada Derena News  (2, 'යළිත් වාහන ආනයනයට ඉඩ ලැබෙන්නේ මෙහෙමයි.. \"...\n",
      "777  Ada Derena News  (2, 'කුරුන්දි විහාරයේ ප්\\u200dරශ්නය දුරදිග ගෙන...\n",
      "778  Ada Derena News  (2, 'Belarus starts taking delivery of Russian...\n",
      "779  Ada Derena News  (2, 'Former MP arrested for firing gun into ai...\n",
      "\n",
      "[780 rows x 2 columns],                 Source_Name                                           Articles\n",
      "0    Ada Derana Sports News  (6, 'කොදෙව් කණ්ඩායමට සුපිරි ජයක්Toggle navigat...\n",
      "1    Ada Derana Sports News  (6, 'වනිඳුට සුපිරි අවස්ථාවක්Toggle navigation ...\n",
      "2    Ada Derana Sports News  (6, 'චමරි අතපත්තු ප්\\u200dරථම ස්ථානයටToggle na...\n",
      "3    Ada Derana Sports News  (6, 'JavaScript is not available.We’ve detecte...\n",
      "4    Ada Derana Sports News  (6, 'කාසියේ වාසිය නවසීලන්තයටToggle navigation ...\n",
      "..                      ...                                                ...\n",
      "337  Ada Derana Sports News  (6, \"FacebookFacebookEmail or phonePasswordFor...\n",
      "338  Ada Derana Sports News  (6, ' What can Dhammika Perera do for his coun...\n",
      "339  Ada Derana Sports News  (6, 'Ada Derana World News Tonight | 07th June...\n",
      "340  Ada Derana Sports News  (6, ' - YouTubeපිළිබඳවමාධ්\\u200dයප්\\u200dරකාශන...\n",
      "341  Ada Derana Sports News  (6, 'ටිකට් ගැන තියන ඔක්කොම ප්\\u200dරශ්ණ වලට උත...\n",
      "\n",
      "[342 rows x 2 columns],            Source_Name                                           Articles\n",
      "0    Ceylon Today News  (7, \"Six killed in stabbing attack at China ki...\n",
      "1    Ceylon Today News  (7, \"Man who attempted to scare off wild eleph...\n",
      "2    Ceylon Today News  (7, \"Body found under mysterious circumstances...\n",
      "3    Ceylon Today News  (7, \"Bus veers off road killing 10, injuring 4...\n",
      "4    Ceylon Today News  (7, \"‘Very important’ revelation in near futur...\n",
      "..                 ...                                                ...\n",
      "993  Ceylon Today News  (7, \"Lorry driver falls asleep at wheel killin...\n",
      "994  Ceylon Today News  (7, \"Litro reduces LP gas prices - Ceylon Toda...\n",
      "995  Ceylon Today News  (7, \"National Flag to be at half-mast tomorrow...\n",
      "996  Ceylon Today News  (7, \"Expressway bus fares reduced by 10% - Cey...\n",
      "997  Ceylon Today News  (7, \"Youth dies after motorcycle crashes into ...\n",
      "\n",
      "[998 rows x 2 columns],                Source_Name                                           Articles\n",
      "0     Colombo Gazette News  (8, 'JavaScript is not available.We’ve detecte...\n",
      "1     Colombo Gazette News  (8, '  Former Minister claims funds from Guate...\n",
      "2     Colombo Gazette News  (8, '  Over 20 injured after bus falls down pr...\n",
      "3     Colombo Gazette News  (8, '  Sri Lanka trounce Netherlands to win CW...\n",
      "4     Colombo Gazette News  (8, '  C.D. Wickramaratne appointed IGP for an...\n",
      "...                    ...                                                ...\n",
      "1000  Colombo Gazette News  (8, '  New SC judge, Appeal Court President, A...\n",
      "1001  Colombo Gazette News  (8, '  Former UN Secretary-General Ban Ki-moon...\n",
      "1002  Colombo Gazette News  (8, '  Man kills his two children and commits ...\n",
      "1003  Colombo Gazette News  (8, '  Political vlogger Darshana Handungoda a...\n",
      "1004  Colombo Gazette News  (8, '  Chandra Schaffter says Dinesh was murde...\n",
      "\n",
      "[1005 rows x 2 columns],            Source_Name                                           Articles\n",
      "0    Daily Mirror News  (1, \"Reflecting… 40th Anniversary of ‘Black Ju...\n",
      "1    Daily Mirror News  (1, \"Inflation is easing, but the poor are lef...\n",
      "2    Daily Mirror News  (1, \"EDUCATION OF CHILDREN  A MUST IN THE PERI...\n",
      "3    Daily Mirror News  (1, 'JavaScript is not available.We’ve detecte...\n",
      "4    Daily Mirror News  (1, \"Ajith Rohana’s FR petition against transf...\n",
      "..                 ...                                                ...\n",
      "784  Daily Mirror News  (1, \"Customs and CTC to destroy largest fag ha...\n",
      "785  Daily Mirror News  (1, \"Felicitation for Master Shi Fa Zhao  - Ca...\n",
      "786  Daily Mirror News  (1, 'Some groups trying to grab power through ...\n",
      "787  Daily Mirror News  (1, \"UNP launches digital app  - Caption Story...\n",
      "788  Daily Mirror News  (1, 'Beware of buying whitening creams online:...\n",
      "\n",
      "[789 rows x 2 columns],             Source_Name                                           Articles\n",
      "0    Lanka Express News  (4, 'JavaScript is not available.We’ve detecte...\n",
      "1    Lanka Express News  (4, 'Thai Air Asia Resumes Flights Connecting ...\n",
      "2    Lanka Express News  (4, '  11 dead, several injured after bus coll...\n",
      "3    Lanka Express News  (4, '  Pakistan Test Team led by Babar Azam ar...\n",
      "4    Lanka Express News  (4, '  Every 40 seconds someone loses their li...\n",
      "..                  ...                                                ...\n",
      "711  Lanka Express News  (4, 'X-Press Pearl Shipping Disaster: Compensa...\n",
      "712  Lanka Express News  (4, 'JavaScript is not available.We’ve detecte...\n",
      "713  Lanka Express News  (4, 'Imported Egg Shipment Update: Clearance E...\n",
      "714  Lanka Express News  (4, '  SL among best places to travel around t...\n",
      "715  Lanka Express News  (4, 'Security measures increased for Easter Su...\n",
      "\n",
      "[716 rows x 2 columns],     Source_Name                                           Articles\n",
      "0      News 1st  (3, 'The London March, dubbed the \"Struggle\" w...\n",
      "1      News 1st  (3, '10 killed and 41 injured in Manampitiya b...\n",
      "2      News 1st  (3, 'Deadline to submit appeals with regard to...\n",
      "3      News 1st  (3, 'Government is scared of another Aragalaya...\n",
      "4      News 1st  (3, \"Vijitha Herath steps down from Select Com...\n",
      "..          ...                                                ...\n",
      "817    News 1st  (3, \"Sri Lanka seeks ADB support in digitaliza...\n",
      "818    News 1st  (3, 'Electricity commission to replace PUCSL?h...\n",
      "819    News 1st  (3, \"Ali Sabry discusses economic partnership ...\n",
      "820    News 1st  (3, \"Dialog & Airtel announce mergerhomeHomeLa...\n",
      "821    News 1st  (3, 'PUCSL Chairman warns of dangerous new Ele...\n",
      "\n",
      "[822 rows x 2 columns],     Source_Name                                           Articles\n",
      "0     News Wire  (5, 'Three lankans in ICC Cricket WC Qualifier...\n",
      "1     News Wire  (5, 'Today’s CBSL official exchange rates - Ne...\n",
      "2     News Wire  (5, 'Russel suggests two key changes to SL squ...\n",
      "3     News Wire  (5, 'Police inquiry over assault on St. Joseph...\n",
      "4     News Wire  (5, 'Dollar rate increases at banks today - Ne...\n",
      "..          ...                                                ...\n",
      "891   News Wire  (5, 'World Rugby suspends Sri Lanka - NewsWire...\n",
      "892   News Wire  (5, 'Rupee appreciates further against USD - N...\n",
      "893   News Wire  (5, \"Nasa reveals why you'll lose weight on a ...\n",
      "894   News Wire  (5, \"X-Press Pearl case : Statement from Presi...\n",
      "895   News Wire  (5, 'Harshana & Mujibur get new positions in S...\n",
      "\n",
      "[896 rows x 2 columns],                Source_Name                                           Articles\n",
      "0    Sri Lanka Mirror News  (9, \"සලාකා ඇතුළු ආයතන 15ක මනි එක්ස්චේන්ජ් ලයිස...\n",
      "1    Sri Lanka Mirror News  (9, \"Prez poll next year –\\xa0Saman Ratnapriya...\n",
      "2    Sri Lanka Mirror News  (9, \"මේ වසර් ඡන්ද නෑ ; ලබන වසරේ ජනාධිපතිවරණයක්...\n",
      "3    Sri Lanka Mirror News  (9, \"එක්ස්ප්\\u200dරස් පර්ල් වන්දි ගෙවීමේ ක්\\u2...\n",
      "4    Sri Lanka Mirror News  (9, \"X-Press Pearl : AG ordered to submit prog...\n",
      "..                     ...                                                ...\n",
      "977  Sri Lanka Mirror News  (9, \"Harsha de Silva named Chairman of the CoP...\n",
      "978  Sri Lanka Mirror News  (9, \"සද්ධාරතන හිමි \\xa021 වනදා දක්වා තවදුරටත් ...\n",
      "979  Sri Lanka Mirror News  (9, \"පගා ඉල්ලූ ප්\\u200dරා.ලේ. අත්අඩංගුවට – Sri...\n",
      "980  Sri Lanka Mirror News  (9, \"Natasha – Bruno further remanded – Sri La...\n",
      "981  Sri Lanka Mirror News  (9, \"නතාෂා – බෲනෝ යළිත් රිමාන්ඩ් – Sri Lanka M...\n",
      "\n",
      "[982 rows x 2 columns],           Source_Name                                           Articles\n",
      "0    The Morning News  (10, 'Committee appointed to review insurance ...\n",
      "1    The Morning News  (10, 'Speeding bus without valid route permit ...\n",
      "2    The Morning News  (10, 'Bus full of pilgrims collides into parke...\n",
      "3    The Morning News  (10, 'Six dead in China kindergarten stabbing ...\n",
      "4    The Morning News  (10, \"President's inaugural visit to India sin...\n",
      "..                ...                                                ...\n",
      "970  The Morning News  (10, ' Special commodity levy imposed on impor...\n",
      "971  The Morning News  (10, '“This happened with full knowledge of Pr...\n",
      "972  The Morning News  (10, 'New LG election dates to be announced on...\n",
      "973  The Morning News  (10, 'HRCSL to look into prevailing shortage o...\n",
      "974  The Morning News  (10, 'Prez calls BASL Prez a ‘political’ lawye...\n",
      "\n",
      "[975 rows x 2 columns]]\n"
     ]
    }
   ],
   "source": [
    "print(lanka_news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w6bwhIwOxDn0"
   },
   "outputs": [],
   "source": [
    "lanka_news_sources_df = pd.concat(lanka_news_sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "umV-bGFIxDn1"
   },
   "outputs": [],
   "source": [
    "from ast import literal_eval as make_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mqoCPP2ExDn1"
   },
   "outputs": [],
   "source": [
    "article_list = []\n",
    "for i in lanka_news_sources_df['Articles'].values.tolist() :\n",
    "    article_list.append(make_tuple(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqfcUvjUxDn1",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "corpus = pd.DataFrame(article_list , columns = ['news_source_id','Articles'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2dGZHSfxDn1",
    "outputId": "4d2ea4e3-8362-445f-98d6-5029c31ea5fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8305 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles\n",
       "0                  2  CBSL decides not to renew permits of 15 money ...\n",
       "1                  2  X-Press Pearl disaster: SC calls for report on...\n",
       "2                  2  Sri Lanka News | Breaking News in Sri Lanka | ...\n",
       "3                  2  Torrential rain, floods kill 22 across norther...\n",
       "4                  2  Driver arrested over bus accident in Manampiti...\n",
       "...              ...                                                ...\n",
       "8300              10   Special commodity levy imposed on imported eg...\n",
       "8301              10  “This happened with full knowledge of Presiden...\n",
       "8302              10  New LG election dates to be announced on March...\n",
       "8303              10  HRCSL to look into prevailing shortage of medi...\n",
       "8304              10  Prez calls BASL Prez a ‘political’ lawyer | Th...\n",
       "\n",
       "[8305 rows x 2 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D5TewL-mxDn2"
   },
   "outputs": [],
   "source": [
    "corpus.to_csv('intermediate_file_1.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R8QbomRIxDn2",
    "outputId": "69fc095c-1874-426e-c171-9dcfe7cba77e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou are given a collection of extracted news articles scraped from Sri Lankan news sources over the past 12 months.\\nClean the dataset by removing null (or nearly empty),\\nnon-English and possible duplicate articles and describe the resulting dataset in terms of the total number\\nof articles and the number of articles from each source.\\n\\nDescribe this cleaned corpus in terms of the total\\nnumber of articles and the number of articles from each source. \\n\\nIn preparation for building a classifier of\\nsuch news articles, address any potential imbalance in the dataset and describe the resulting final dataset\\nin terms of the total number of articles and the number of articles from each source. Pre-process the corpus\\nusing the standard pipeline and describe the final dataset in terms the total number of tokens and the number\\nof unique tokens in the full corpus.\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You are given a collection of extracted news articles scraped from Sri Lankan news sources over the past 12 months.\n",
    "Clean the dataset by removing null (or nearly empty),\n",
    "non-English and possible duplicate articles and describe the resulting dataset in terms of the total number\n",
    "of articles and the number of articles from each source.\n",
    "\n",
    "Describe this cleaned corpus in terms of the total\n",
    "number of articles and the number of articles from each source.\n",
    "\n",
    "In preparation for building a classifier of\n",
    "such news articles, address any potential imbalance in the dataset and describe the resulting final dataset\n",
    "in terms of the total number of articles and the number of articles from each source. Pre-process the corpus\n",
    "using the standard pipeline and describe the final dataset in terms the total number of tokens and the number\n",
    "of unique tokens in the full corpus.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zq5F4c90xDn2"
   },
   "source": [
    "## Analyse the News articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eegxCYuTxDn2"
   },
   "source": [
    "How to analyse the documents under each source ?\n",
    "\n",
    "In text analytics before applying the clustering algorithms we do a data profile check on the articles we have. This involves understanding the characteristics and patterns of the data.   Initially we are curious to check whether there are null or nearly empty, non-English and possible duplicated articles. <br>\n",
    "\n",
    "(1). Document length distribution --> Examine the distribution of document lengths to understand the variation in document sizes. <br>\n",
    "(2). Domain Specific analyses --> Consider domain-specific analyses to gain insights into the context of the documents.\n",
    "Identify key terms or topics that are relevant to the domain.<br>\n",
    "(3). Data Exploration --> Check for any outliers or anomalies in the data.Explore basic statistics such as document length, word frequency, and vocabulary size.<br>\n",
    "(4). Text preprocessing --> Tokenize the text to break it down into words or phrases.\n",
    "Remove stop words (common words that don't carry much meaning) and punctuation.\n",
    "Perform stemming or lemmatization to reduce words to their base form.<br>\n",
    "(5). Feature Extraction --> Convert the text data into a numerical format suitable for clustering algorithms.\n",
    "Use techniques like TF-IDF (Term Frequency-Inverse Document Frequency) to represent the importance of words in the documents.<br>\n",
    "(6). Term Frequency Analysis--> Analyze the term frequency distribution to identify frequently occurring words.\n",
    "Plot word clouds or histograms to visualize the most common terms.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2_s3asfaxDn2"
   },
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aF1-3QfoxDn2",
    "outputId": "78bb294e-a921-462e-b901-dc7ad2b4caf4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_source_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Articles\n",
       "news_source_id          \n",
       "1                    789\n",
       "2                    780\n",
       "3                    822\n",
       "4                    716\n",
       "5                    896\n",
       "6                    342\n",
       "7                    998\n",
       "8                   1005\n",
       "9                    982\n",
       "10                   975"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.groupby(['news_source_id']).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8iJ_rPo4xDn2"
   },
   "source": [
    "### 1. Remove non english"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the data, observed some html embedded strings , non english content and time stamps. It was considered as the first step, because if we remove the empty articles or nearly empty articles, we will miss the articles where we already clean by this step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G2lPwxujxDn3"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def is_english(text):\n",
    "    english_pattern = re.compile(r'[^a-zA-Z\\s]')\n",
    "    return re.sub(english_pattern, '', text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_markup(text):\n",
    "    # Remove HTML markup\n",
    "    text = re.sub('<.*?>', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mB0mFvf8xDn3"
   },
   "outputs": [],
   "source": [
    "def show_non_english(text):\n",
    "    english_pattern = re.compile(r'[a-zA-Z\\s]')\n",
    "    return re.sub(english_pattern, '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5ERuKM2xDn3"
   },
   "outputs": [],
   "source": [
    "def remove_non_english_articles(df, col = ''):\n",
    "    # Remove non-English article\n",
    "    df[col] = df[col].apply(lambda x: remove_html_markup(x))\n",
    "    df['non_english'] = df[col].apply(lambda x: show_non_english(x))\n",
    "    df['english_context'] = df[col].apply(lambda x: is_english(x))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XR3DeT93xDn3"
   },
   "outputs": [],
   "source": [
    "corpus = remove_non_english_articles(corpus , 'Articles')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "usQnzhsvxDn3"
   },
   "source": [
    "Here we detect the regex pattern [^a-zA-Z0-9.,!?;:()\"'\\s] which matches any character that is not an English alphabet, number, common English punctuation, or whitespace. The re.sub function is then used to replace those non-English characters with an empty string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "52UUrUHexDn3",
    "outputId": "f1a8a355-bfcd-47b9-f75b-dd73ae43c477"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>non_english</th>\n",
       "      <th>english_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "      <td>1510,20238:351510,202303:27()15()2023.,-2022.,...</td>\n",
       "      <td>CBSL decides not to renew permits of  money ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "      <td>-:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...</td>\n",
       "      <td>XPress Pearl disaster SC calls for report on c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "      <td>||-10,20238:3510(09),10...|10,202311:2410(09),...</td>\n",
       "      <td>Sri Lanka News  Breaking News in Sri Lanka  Ad...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "      <td>,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...</td>\n",
       "      <td>Torrential rain floods kill  across northern I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "      <td>“”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...</td>\n",
       "      <td>This happened with full knowledge of President...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "      <td>‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....</td>\n",
       "      <td>Prez calls BASL Prez a political lawyer  The M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8305 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles  \\\n",
       "0                  2  CBSL decides not to renew permits of 15 money ...   \n",
       "1                  2  X-Press Pearl disaster: SC calls for report on...   \n",
       "2                  2  Sri Lanka News | Breaking News in Sri Lanka | ...   \n",
       "3                  2  Torrential rain, floods kill 22 across norther...   \n",
       "4                  2  Driver arrested over bus accident in Manampiti...   \n",
       "...              ...                                                ...   \n",
       "8300              10   Special commodity levy imposed on imported eg...   \n",
       "8301              10  “This happened with full knowledge of Presiden...   \n",
       "8302              10  New LG election dates to be announced on March...   \n",
       "8303              10  HRCSL to look into prevailing shortage of medi...   \n",
       "8304              10  Prez calls BASL Prez a ‘political’ lawyer | Th...   \n",
       "\n",
       "                                            non_english  \\\n",
       "0     1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
       "1     -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
       "2     ||-10,20238:3510(09),10...|10,202311:2410(09),...   \n",
       "3     ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
       "4     1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...   \n",
       "...                                                 ...   \n",
       "8300  |10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...   \n",
       "8301  “”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...   \n",
       "8302  03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...   \n",
       "8303  |10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...   \n",
       "8304  ‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....   \n",
       "\n",
       "                                        english_context  \n",
       "0     CBSL decides not to renew permits of  money ch...  \n",
       "1     XPress Pearl disaster SC calls for report on c...  \n",
       "2     Sri Lanka News  Breaking News in Sri Lanka  Ad...  \n",
       "3     Torrential rain floods kill  across northern I...  \n",
       "4     Driver arrested over bus accident in Manampiti...  \n",
       "...                                                 ...  \n",
       "8300   Special commodity levy imposed on imported eg...  \n",
       "8301  This happened with full knowledge of President...  \n",
       "8302  New LG election dates to be announced on March...  \n",
       "8303  HRCSL to look into prevailing shortage of medi...  \n",
       "8304  Prez calls BASL Prez a political lawyer  The M...  \n",
       "\n",
       "[8305 rows x 4 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bPA_yhg6xDn3"
   },
   "source": [
    "### 2. Remove null records or nearly empty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To remove nearly empty records, I considered that there should be atleast with the noice data even after cleaning inappropriate content, we need to have atleast the word count 15. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UpOab-sBxDn3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def clear_null_records_or_nearly_empty(df , col = ''):\n",
    "    # Remove null or nearly empty articles\n",
    "    df.dropna(subset=[col], inplace=True)\n",
    "\n",
    "    # Assuming we want to remove articles with less than 10 words (we can adjust the threshold)\n",
    "    df['word_count'] = df[col].apply(lambda x: len(str(x).split()))\n",
    "    df = df[df['word_count'] >= 15]\n",
    "\n",
    "\n",
    "    # Display information after removing null or nearly empty articles\n",
    "    print(\"\\nDataset Information After Removing Null or Nearly Empty Articles:\")\n",
    "    print(df.info())\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KM5gExfxxDn3",
    "outputId": "0e11619e-48f9-4bf6-b373-50d443d8e438"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Information After Removing Null or Nearly Empty Articles:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7549 entries, 0 to 8304\n",
      "Data columns (total 5 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   news_source_id   7549 non-null   int64 \n",
      " 1   Articles         7549 non-null   object\n",
      " 2   non_english      7549 non-null   object\n",
      " 3   english_context  7549 non-null   object\n",
      " 4   word_count       7549 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 353.9+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "corpus = clear_null_records_or_nearly_empty(corpus , 'english_context')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2oB6sCTcxDn3",
    "outputId": "63c4195c-2fd9-4618-c2a7-72d5d6bb04d8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>non_english</th>\n",
       "      <th>english_context</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "      <td>1510,20238:351510,202303:27()15()2023.,-2022.,...</td>\n",
       "      <td>CBSL decides not to renew permits of  money ch...</td>\n",
       "      <td>575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "      <td>-:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...</td>\n",
       "      <td>XPress Pearl disaster SC calls for report on c...</td>\n",
       "      <td>511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "      <td>||-10,20238:3510(09),10...|10,202311:2410(09),...</td>\n",
       "      <td>Sri Lanka News  Breaking News in Sri Lanka  Ad...</td>\n",
       "      <td>4048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "      <td>,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...</td>\n",
       "      <td>Torrential rain floods kill  across northern I...</td>\n",
       "      <td>553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "      <td>“”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...</td>\n",
       "      <td>This happened with full knowledge of President...</td>\n",
       "      <td>445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "      <td>‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....</td>\n",
       "      <td>Prez calls BASL Prez a political lawyer  The M...</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7549 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles  \\\n",
       "0                  2  CBSL decides not to renew permits of 15 money ...   \n",
       "1                  2  X-Press Pearl disaster: SC calls for report on...   \n",
       "2                  2  Sri Lanka News | Breaking News in Sri Lanka | ...   \n",
       "3                  2  Torrential rain, floods kill 22 across norther...   \n",
       "4                  2  Driver arrested over bus accident in Manampiti...   \n",
       "...              ...                                                ...   \n",
       "8300              10   Special commodity levy imposed on imported eg...   \n",
       "8301              10  “This happened with full knowledge of Presiden...   \n",
       "8302              10  New LG election dates to be announced on March...   \n",
       "8303              10  HRCSL to look into prevailing shortage of medi...   \n",
       "8304              10  Prez calls BASL Prez a ‘political’ lawyer | Th...   \n",
       "\n",
       "                                            non_english  \\\n",
       "0     1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
       "1     -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
       "2     ||-10,20238:3510(09),10...|10,202311:2410(09),...   \n",
       "3     ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
       "4     1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...   \n",
       "...                                                 ...   \n",
       "8300  |10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...   \n",
       "8301  “”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...   \n",
       "8302  03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...   \n",
       "8303  |10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...   \n",
       "8304  ‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....   \n",
       "\n",
       "                                        english_context  word_count  \n",
       "0     CBSL decides not to renew permits of  money ch...         575  \n",
       "1     XPress Pearl disaster SC calls for report on c...         511  \n",
       "2     Sri Lanka News  Breaking News in Sri Lanka  Ad...        4048  \n",
       "3     Torrential rain floods kill  across northern I...         553  \n",
       "4     Driver arrested over bus accident in Manampiti...         518  \n",
       "...                                                 ...         ...  \n",
       "8300   Special commodity levy imposed on imported eg...         215  \n",
       "8301  This happened with full knowledge of President...         445  \n",
       "8302  New LG election dates to be announced on March...         192  \n",
       "8303  HRCSL to look into prevailing shortage of medi...         187  \n",
       "8304  Prez calls BASL Prez a political lawyer  The M...         390  \n",
       "\n",
       "[7549 rows x 5 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SgtR5HaxDn4"
   },
   "outputs": [],
   "source": [
    "corpus.to_csv('nearlu_empty.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NIRV3iw8xDn4",
    "outputId": "77184a6b-3cb7-4808-c6c0-8e354fd5e8fa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['news_source_id', 'Articles', 'non_english', 'english_context',\n",
       "       'word_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XqKCtD_xDn4"
   },
   "source": [
    "### 3. Remove duplicated using Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tdjZWdbxDn4"
   },
   "source": [
    "Removing duplicates in a corpus using Natural Language Processing (NLP) involves comparing and identifying similar or identical text documents. The common approach is by using similarity measures and tokenization. We'll use the spaCy library for tokenization and the TfidfVectorizer from scikit-learn for creating document vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tGNP_diHxDn4"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IzTKLvL5xDn5"
   },
   "source": [
    "## Pre-process the corpus using the standard pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to use cosine similarity score we need to prepare the set of unique tokens apply for each article. Therefore we need to tokenize the articles and convert it to vectors for easy execution. Therefore, data preprocessing will be done here itself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5giH63MSxDn_"
   },
   "source": [
    "#### Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cMqFX3N5xDn_"
   },
   "outputs": [],
   "source": [
    "from contractions import CONTRACTION_MAP\n",
    "import re\n",
    "import nltk\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AybKie2OxDn_"
   },
   "outputs": [],
   "source": [
    "stopword_list = nltk.corpus.stopwords.words('english')\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elNLSAmzxDoA"
   },
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return tokens\n",
    "\n",
    "def expand_contractions(text, contraction_mapping):\n",
    "\n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())),\n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "    def expand_match(contraction):\n",
    "        match = contraction.group(0)\n",
    "        first_char = match[0]\n",
    "        expanded_contraction = contraction_mapping.get(match)\\\n",
    "                                if contraction_mapping.get(match)\\\n",
    "                                else contraction_mapping.get(match.lower())\n",
    "        expanded_contraction = first_char+expanded_contraction[1:]\n",
    "        return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    expanded_text = re.sub(\"'\", \"\", expanded_text)\n",
    "    return expanded_text\n",
    "\n",
    "\n",
    "# from pattern.en import tag (if using python 2.x)\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "\n",
    "# Annotate text tokens with POS tags\n",
    "def pos_tag_text(text):\n",
    "\n",
    "    def penn_to_wn_tags(pos_tag):\n",
    "        if pos_tag.startswith('J'):\n",
    "            return wn.ADJ\n",
    "        elif pos_tag.startswith('V'):\n",
    "            return wn.VERB\n",
    "        elif pos_tag.startswith('N'):\n",
    "            return wn.NOUN\n",
    "        elif pos_tag.startswith('R'):\n",
    "            return wn.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tagged_text = nltk.pos_tag(tokens)\n",
    "\n",
    "#    tagged_text = tag(text) # If using pattern.en in python 2.x\n",
    "    tagged_lower_text = [(word.lower(), penn_to_wn_tags(pos_tag))\n",
    "                         for word, pos_tag in\n",
    "                         tagged_text]\n",
    "\n",
    "    return tagged_lower_text\n",
    "\n",
    "# Lemmatize text based on POS tags using Wordnet\n",
    "def lemmatize_text(text):\n",
    "\n",
    "    pos_tagged_text = pos_tag_text(text)\n",
    "    lemmatized_tokens = [wnl.lemmatize(word, pos_tag) if pos_tag\n",
    "                         else word\n",
    "                         for word, pos_tag in pos_tagged_text]\n",
    "    lemmatized_text = ' '.join(lemmatized_tokens)\n",
    "    return lemmatized_text\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    pattern = re.compile('[{}]'.format(re.escape(string.punctuation)))\n",
    "    filtered_tokens = filter(None, [pattern.sub('', token) for token in tokens])\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    tokens = tokenize_text(text)\n",
    "    filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "    filtered_text = ' '.join(filtered_tokens)\n",
    "    return filtered_text\n",
    "\n",
    "\n",
    "# Do all the pre-processing needed for the corpus\n",
    "# Calls all the above functions to achieve this\n",
    "\n",
    "def normalize_corpus(text):\n",
    "    \n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = tokenize_text(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83iMm2kexDoA",
    "outputId": "4e2edcb0-b3d6-4361-ab32-af25c9daea3c"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hakim\\AppData\\Local\\Temp\\ipykernel_2200\\2351256535.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus['preprocessed_article'] = corpus['english_context'].apply(lambda x : normalize_corpus(x))\n"
     ]
    }
   ],
   "source": [
    "corpus['preprocessed_article'] = corpus['english_context'].apply(lambda x : normalize_corpus(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaBB2upExDoA"
   },
   "outputs": [],
   "source": [
    "corpus.to_csv('preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0i_iFxv7xDoA"
   },
   "outputs": [],
   "source": [
    "preprocessed_corpus = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2eTfnrTkxDoA",
    "outputId": "4bd7831c-054d-4945-84ae-72384f216edb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>non_english</th>\n",
       "      <th>english_context</th>\n",
       "      <th>word_count</th>\n",
       "      <th>preprocessed_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "      <td>1510,20238:351510,202303:27()15()2023.,-2022.,...</td>\n",
       "      <td>CBSL decides not to renew permits of  money ch...</td>\n",
       "      <td>575</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "      <td>-:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...</td>\n",
       "      <td>XPress Pearl disaster SC calls for report on c...</td>\n",
       "      <td>511</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "      <td>||-10,20238:3510(09),10...|10,202311:2410(09),...</td>\n",
       "      <td>Sri Lanka News  Breaking News in Sri Lanka  Ad...</td>\n",
       "      <td>4048</td>\n",
       "      <td>['sri', 'lanka', 'news', 'breaking', 'news', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "      <td>,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...</td>\n",
       "      <td>Torrential rain floods kill  across northern I...</td>\n",
       "      <td>553</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>518</td>\n",
       "      <td>['driver', 'arrest', 'bus', 'accident', 'manam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>8300</td>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>215</td>\n",
       "      <td>['special', 'commodity', 'levy', 'impose', 'im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>8301</td>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "      <td>“”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...</td>\n",
       "      <td>This happened with full knowledge of President...</td>\n",
       "      <td>445</td>\n",
       "      <td>['happen', 'full', 'knowledge', 'president', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>8302</td>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>192</td>\n",
       "      <td>['new', 'lg', 'election', 'date', 'announce', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>8303</td>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>187</td>\n",
       "      <td>['hrcsl', 'look', 'prevailing', 'shortage', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>8304</td>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "      <td>‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....</td>\n",
       "      <td>Prez calls BASL Prez a political lawyer  The M...</td>\n",
       "      <td>390</td>\n",
       "      <td>['prez', 'call', 'basl', 'prez', 'political', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7549 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  news_source_id  \\\n",
       "0              0               2   \n",
       "1              1               2   \n",
       "2              2               2   \n",
       "3              3               2   \n",
       "4              4               2   \n",
       "...          ...             ...   \n",
       "7544        8300              10   \n",
       "7545        8301              10   \n",
       "7546        8302              10   \n",
       "7547        8303              10   \n",
       "7548        8304              10   \n",
       "\n",
       "                                               Articles  \\\n",
       "0     CBSL decides not to renew permits of 15 money ...   \n",
       "1     X-Press Pearl disaster: SC calls for report on...   \n",
       "2     Sri Lanka News | Breaking News in Sri Lanka | ...   \n",
       "3     Torrential rain, floods kill 22 across norther...   \n",
       "4     Driver arrested over bus accident in Manampiti...   \n",
       "...                                                 ...   \n",
       "7544   Special commodity levy imposed on imported eg...   \n",
       "7545  “This happened with full knowledge of Presiden...   \n",
       "7546  New LG election dates to be announced on March...   \n",
       "7547  HRCSL to look into prevailing shortage of medi...   \n",
       "7548  Prez calls BASL Prez a ‘political’ lawyer | Th...   \n",
       "\n",
       "                                            non_english  \\\n",
       "0     1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
       "1     -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
       "2     ||-10,20238:3510(09),10...|10,202311:2410(09),...   \n",
       "3     ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
       "4     1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...   \n",
       "...                                                 ...   \n",
       "7544  |10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...   \n",
       "7545  “”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...   \n",
       "7546  03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...   \n",
       "7547  |10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...   \n",
       "7548  ‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....   \n",
       "\n",
       "                                        english_context  word_count  \\\n",
       "0     CBSL decides not to renew permits of  money ch...         575   \n",
       "1     XPress Pearl disaster SC calls for report on c...         511   \n",
       "2     Sri Lanka News  Breaking News in Sri Lanka  Ad...        4048   \n",
       "3     Torrential rain floods kill  across northern I...         553   \n",
       "4     Driver arrested over bus accident in Manampiti...         518   \n",
       "...                                                 ...         ...   \n",
       "7544   Special commodity levy imposed on imported eg...         215   \n",
       "7545  This happened with full knowledge of President...         445   \n",
       "7546  New LG election dates to be announced on March...         192   \n",
       "7547  HRCSL to look into prevailing shortage of medi...         187   \n",
       "7548  Prez calls BASL Prez a political lawyer  The M...         390   \n",
       "\n",
       "                                   preprocessed_article  \n",
       "0     ['cbsl', 'decide', 'renew', 'permit', 'money',...  \n",
       "1     ['xpress', 'pearl', 'disaster', 'sc', 'call', ...  \n",
       "2     ['sri', 'lanka', 'news', 'breaking', 'news', '...  \n",
       "3     ['torrential', 'rain', 'flood', 'kill', 'acros...  \n",
       "4     ['driver', 'arrest', 'bus', 'accident', 'manam...  \n",
       "...                                                 ...  \n",
       "7544  ['special', 'commodity', 'levy', 'impose', 'im...  \n",
       "7545  ['happen', 'full', 'knowledge', 'president', '...  \n",
       "7546  ['new', 'lg', 'election', 'date', 'announce', ...  \n",
       "7547  ['hrcsl', 'look', 'prevailing', 'shortage', 'm...  \n",
       "7548  ['prez', 'call', 'basl', 'prez', 'political', ...  \n",
       "\n",
       "[7549 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0VvTLBw4xDoB"
   },
   "source": [
    "### vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QY7mf0zmxDoB"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CsSYEz5jxDoB"
   },
   "outputs": [],
   "source": [
    "# Create document vectors using TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "document_vectors = vectorizer.fit_transform(preprocessed_corpus['preprocessed_article'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "agXmzFHtxDoB",
    "outputId": "19ca8cc6-83e3-4970-b5a4-6a593e8f2b3e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 25390)\t0.028450965134936432\n",
      "  (0, 49872)\t0.02101600597108272\n",
      "  (0, 68690)\t0.02670228817975667\n",
      "  (0, 58307)\t0.03188554928242011\n",
      "  (0, 3205)\t0.03196675793906328\n",
      "  (0, 52109)\t0.03196675793906328\n",
      "  (0, 73470)\t0.03196675793906328\n",
      "  (0, 7860)\t0.03196675793906328\n",
      "  (0, 73343)\t0.03196675793906328\n",
      "  (0, 45226)\t0.03196675793906328\n",
      "  (0, 6682)\t0.03196675793906328\n",
      "  (0, 6681)\t0.03196675793906328\n",
      "  (0, 24893)\t0.05008449058167206\n",
      "  (0, 58708)\t0.06144283730582844\n",
      "  (0, 5324)\t0.04425766860807715\n",
      "  (0, 14974)\t0.06377109856484021\n",
      "  (0, 53184)\t0.048770657384059744\n",
      "  (0, 24725)\t0.05561642723586547\n",
      "  (0, 41873)\t0.06377109856484021\n",
      "  (0, 17604)\t0.05249684569453756\n",
      "  (0, 646)\t0.044278962954361824\n",
      "  (0, 66536)\t0.0449301075303929\n",
      "  (0, 70998)\t0.05404308986567626\n",
      "  (0, 61855)\t0.036752954381520625\n",
      "  (0, 75716)\t0.06377109856484021\n",
      "  :\t:\n",
      "  (7548, 42217)\t0.021600398410549768\n",
      "  (7548, 38227)\t0.03213302995950572\n",
      "  (7548, 24452)\t0.040552048881110914\n",
      "  (7548, 37947)\t0.4663425344425267\n",
      "  (7548, 26606)\t0.024441672251067866\n",
      "  (7548, 3697)\t0.035754310746103225\n",
      "  (7548, 5863)\t0.04322763372858743\n",
      "  (7548, 43557)\t0.08729111959893077\n",
      "  (7548, 9922)\t0.05824412619343028\n",
      "  (7548, 35159)\t0.014682607455250381\n",
      "  (7548, 45137)\t0.027802854812547753\n",
      "  (7548, 41153)\t0.023270216544649207\n",
      "  (7548, 9456)\t0.053381817943176486\n",
      "  (7548, 74758)\t0.032837038477762835\n",
      "  (7548, 59084)\t0.034716904676346465\n",
      "  (7548, 12862)\t0.026442767298624523\n",
      "  (7548, 12625)\t0.01515718126463928\n",
      "  (7548, 39363)\t0.03978844401132674\n",
      "  (7548, 54986)\t0.08977369625161628\n",
      "  (7548, 61182)\t0.051491444505029224\n",
      "  (7548, 40867)\t0.024341040663034896\n",
      "  (7548, 68225)\t0.01697535059837827\n",
      "  (7548, 56776)\t0.026817238410837517\n",
      "  (7548, 37005)\t0.012131556444744071\n",
      "  (7548, 65506)\t0.012031149430847047\n"
     ]
    }
   ],
   "source": [
    "print(document_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2vUGMfdXxDoC"
   },
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cosine_similarity score says , when we pass two sentences or articles, how similar (returns percentage value)  are those articles are. Here when filtering out the duplicate documents,with considering noise data, to remove the duplicates atleast the score should be more than 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCLgX5X5xDoC"
   },
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between document vectors\n",
    "cosine_similarities = cosine_similarity(document_vectors, document_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rA_iBWT3xDoC",
    "outputId": "aa154be8-c4c4-4db9-8824-6142955c5923"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7549, 7549)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosine_similarities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQVICKM7xDoC"
   },
   "outputs": [],
   "source": [
    "# Threshold for considering documents as duplicates (adjust as needed)\n",
    "duplicate_threshold = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wK6ySWboxDoC",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Identify and remove duplicates\n",
    "indices_to_remove = set()\n",
    "for i in range(len(preprocessed_corpus)):\n",
    "\n",
    "    if i not in indices_to_remove:\n",
    "        similar_indices = set(index for index, score in enumerate(cosine_similarities[i]) if score > duplicate_threshold)\n",
    "\n",
    "        if len(similar_indices) > 1 :\n",
    "            indices_to_remove.update(similar_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvKAAeDBxDoC",
    "outputId": "5ad37fad-74ee-426e-faf0-37327f647c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset without duplicates:\n",
      "   Unnamed: 0  news_source_id  \\\n",
      "0           0               2   \n",
      "1           1               2   \n",
      "3           3               2   \n",
      "5           5               2   \n",
      "7          12               2   \n",
      "\n",
      "                                            Articles  \\\n",
      "0  CBSL decides not to renew permits of 15 money ...   \n",
      "1  X-Press Pearl disaster: SC calls for report on...   \n",
      "3  Torrential rain, floods kill 22 across norther...   \n",
      "5   India says it will continue to support Sri La...   \n",
      "7  Pope Francis names 21 new cardinalsJuly 10, 20...   \n",
      "\n",
      "                                         non_english  \\\n",
      "0  1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
      "1  -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
      "3  ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
      "5  -|සිංහල,10,2023’.---\"\"'.,2023,’.--.“.-,”.().,1...   \n",
      "7  2110,20238:362110,202312:1821,——..’,.30.,,,á,5...   \n",
      "\n",
      "                                     english_context  word_count  \\\n",
      "0  CBSL decides not to renew permits of  money ch...        4363   \n",
      "1  XPress Pearl disaster SC calls for report on c...        3808   \n",
      "3  Torrential rain floods kill  across northern I...        4184   \n",
      "5   India says it will continue to support Sri La...        4254   \n",
      "7  Pope Francis names  new cardinalsJuly    pm To...        8725   \n",
      "\n",
      "                                preprocessed_article  \n",
      "0  ['cbsl', 'decide', 'renew', 'permit', 'money',...  \n",
      "1  ['xpress', 'pearl', 'disaster', 'sc', 'call', ...  \n",
      "3  ['torrential', 'rain', 'flood', 'kill', 'acros...  \n",
      "5  ['india', 'say', 'continue', 'support', 'sri',...  \n",
      "7  ['pope', 'francis', 'names', 'new', 'cardinals...  \n"
     ]
    }
   ],
   "source": [
    "# Drop duplicate indices\n",
    "df_no_duplicates = preprocessed_corpus.drop(index=indices_to_remove)\n",
    "\n",
    "# Display the resulting dataset without duplicates\n",
    "print(\"Dataset without duplicates:\")\n",
    "print(df_no_duplicates.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVvTVMf5xDoC",
    "outputId": "e92d5e0c-2d4e-41bb-9619-29086d69c493"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Unnamed: 0', 'news_source_id', 'Articles', 'non_english',\n",
       "       'english_context', 'word_count', 'preprocessed_article'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_duplicates.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hRaQXrJexDoC"
   },
   "outputs": [],
   "source": [
    "corpus_preprocessed = df_no_duplicates[['news_source_id' , 'preprocessed_article']].rename(columns={'preprocessed_article': 'Articles'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Idxis8DKxDoC",
    "outputId": "432bbd14-5cc5-4b3c-cddd-41c7cb6e5336"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>['india', 'say', 'continue', 'support', 'sri',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>['pope', 'francis', 'names', 'new', 'cardinals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>10</td>\n",
       "      <td>['special', 'commodity', 'levy', 'impose', 'im...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>10</td>\n",
       "      <td>['happen', 'full', 'knowledge', 'president', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>10</td>\n",
       "      <td>['new', 'lg', 'election', 'date', 'announce', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>10</td>\n",
       "      <td>['hrcsl', 'look', 'prevailing', 'shortage', 'm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>10</td>\n",
       "      <td>['prez', 'call', 'basl', 'prez', 'political', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4585 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles\n",
       "0                  2  ['cbsl', 'decide', 'renew', 'permit', 'money',...\n",
       "1                  2  ['xpress', 'pearl', 'disaster', 'sc', 'call', ...\n",
       "3                  2  ['torrential', 'rain', 'flood', 'kill', 'acros...\n",
       "5                  2  ['india', 'say', 'continue', 'support', 'sri',...\n",
       "7                  2  ['pope', 'francis', 'names', 'new', 'cardinals...\n",
       "...              ...                                                ...\n",
       "7544              10  ['special', 'commodity', 'levy', 'impose', 'im...\n",
       "7545              10  ['happen', 'full', 'knowledge', 'president', '...\n",
       "7546              10  ['new', 'lg', 'election', 'date', 'announce', ...\n",
       "7547              10  ['hrcsl', 'look', 'prevailing', 'shortage', 'm...\n",
       "7548              10  ['prez', 'call', 'basl', 'prez', 'political', ...\n",
       "\n",
       "[4585 rows x 2 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ZM5_6A4xDoC"
   },
   "outputs": [],
   "source": [
    "corpus_preprocessed.to_csv('final_preprocessed.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c4XvD4c7xDoD"
   },
   "outputs": [],
   "source": [
    "corpus_preprocessed_ = pd.read_csv('final_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOD1FqeGxDoD"
   },
   "source": [
    "Describe the resulting dataset in terms of the total number of articles and the number of articles from each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hyczj0INxDoD",
    "outputId": "a46d5224-0868-4cc7-e17c-84de447cbd0c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "news_source_id    4585\n",
       "Articles          4585\n",
       "dtype: int64"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_preprocessed_.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LX0Ozwh_xDoD"
   },
   "source": [
    "There are 4585 articles in total in the corpus after removing the duplicates. To observe or detect imbalance data, we will check how the distribution of count of articles from each source,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d4vKw2ZLxDoD",
    "outputId": "e026c511-4e61-463f-eec4-1002d888fee8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_source_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Articles\n",
       "news_source_id          \n",
       "1                    531\n",
       "2                    377\n",
       "3                    464\n",
       "4                    170\n",
       "5                    655\n",
       "6                     24\n",
       "7                    535\n",
       "8                    901\n",
       "9                     87\n",
       "10                   841"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_corpus = corpus_preprocessed_.groupby(['news_source_id']).count()\n",
    "agg_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By identifying the outliers we consider the clusters which have article count out of the range of Q1 and Q2. Then to the resampling size, by calculating the trimmed mean.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fr_4IeLCxDoD",
    "outputId": "9490c468-2bc0-408c-c876-a7e0b7621b36"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "458.5"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_corpus.Articles.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AM6J5E-AxDoD",
    "outputId": "6eedb98e-061a-453c-a53b-8e168bbc8741"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([457.5])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import stats\n",
    "#calculate 10% trimmed mean\n",
    "stats.trim_mean(agg_corpus, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "au9m6sa8xDoE",
    "outputId": "5e3e6138-9997-458c-8130-a29f764d81ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([457.5])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trimmed_mean  = stats.trim_mean(agg_corpus, 0.1)\n",
    "trimmed_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FWQbc658xDoF",
    "outputId": "5e22d82f-b83a-4cb5-edea-26cd2bed5abc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First quatile :  273.5\n",
      "Third quatile :  595.0\n",
      "Interquartile 321.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hakim\\AppData\\Local\\Temp\\ipykernel_2200\\1354273021.py:2: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  Q1 = np.percentile(agg_corpus, 25, interpolation = 'midpoint')\n",
      "C:\\Users\\hakim\\AppData\\Local\\Temp\\ipykernel_2200\\1354273021.py:5: DeprecationWarning: the `interpolation=` argument to percentile was renamed to `method=`, which has additional options.\n",
      "Users of the modes 'nearest', 'lower', 'higher', or 'midpoint' are encouraged to review the method they used. (Deprecated NumPy 1.22)\n",
      "  Q3 = np.percentile(agg_corpus, 75, interpolation = 'midpoint')\n"
     ]
    }
   ],
   "source": [
    "# First quartile (Q1)\n",
    "Q1 = np.percentile(agg_corpus, 25, interpolation = 'midpoint')\n",
    "print('First quatile : ' , Q1)\n",
    "# Third quartile (Q3)\n",
    "Q3 = np.percentile(agg_corpus, 75, interpolation = 'midpoint')\n",
    "print('Third quatile : ' , Q3)\n",
    "# Interquaritle range (IQR)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "print('Interquartile',IQR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FInF6D6VxDoF"
   },
   "source": [
    "In preparation for building a classifier of\n",
    "such news articles, address any potential imbalance in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KWj1HBhUxDoG"
   },
   "source": [
    "Tried to import imblearn to get SMOTE , but it didn't work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QtacJZTZxDoG"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jv8dhgKwxDoG"
   },
   "source": [
    "### Identify undersampled and oversampled clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BztwmUAxxDoG",
    "outputId": "4efd1a1e-d0d6-420d-816f-f8bf581b4646"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upper_boundary :  595.0  lower_boundary: 273.5\n"
     ]
    }
   ],
   "source": [
    "agg_corpus['diff'] = agg_corpus['Articles'].apply(lambda x : trimmed_mean - x)\n",
    "upper_boundary =Q3\n",
    "lower_boundary =Q1\n",
    "print('upper_boundary : ' ,upper_boundary , ' lower_boundary:' , lower_boundary)\n",
    "agg_corpus['cluster_status'] = (agg_corpus['Articles'].apply(\n",
    "                                                        lambda x : \"Undersampled\"\n",
    "                                                        if x < lower_boundary\n",
    "                                                        else (\n",
    "                                                            \"Sampled_enough\"\n",
    "                                                            if ((lower_boundary <= x) and (x <=upper_boundary  ))\n",
    "                                                            else \"Oversampled\") )\n",
    "                                                                                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A_fgrWFGxDoH",
    "outputId": "82d6461f-fc17-46b6-d331-2fd9b2d1f920"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "      <th>diff</th>\n",
       "      <th>cluster_status</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_source_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531</td>\n",
       "      <td>[-73.5]</td>\n",
       "      <td>Sampled_enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>377</td>\n",
       "      <td>[80.5]</td>\n",
       "      <td>Sampled_enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>464</td>\n",
       "      <td>[-6.5]</td>\n",
       "      <td>Sampled_enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>170</td>\n",
       "      <td>[287.5]</td>\n",
       "      <td>Undersampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>655</td>\n",
       "      <td>[-197.5]</td>\n",
       "      <td>Oversampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>24</td>\n",
       "      <td>[433.5]</td>\n",
       "      <td>Undersampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>535</td>\n",
       "      <td>[-77.5]</td>\n",
       "      <td>Sampled_enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>901</td>\n",
       "      <td>[-443.5]</td>\n",
       "      <td>Oversampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>87</td>\n",
       "      <td>[370.5]</td>\n",
       "      <td>Undersampled</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>841</td>\n",
       "      <td>[-383.5]</td>\n",
       "      <td>Oversampled</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Articles      diff  cluster_status\n",
       "news_source_id                                    \n",
       "1                    531   [-73.5]  Sampled_enough\n",
       "2                    377    [80.5]  Sampled_enough\n",
       "3                    464    [-6.5]  Sampled_enough\n",
       "4                    170   [287.5]    Undersampled\n",
       "5                    655  [-197.5]     Oversampled\n",
       "6                     24   [433.5]    Undersampled\n",
       "7                    535   [-77.5]  Sampled_enough\n",
       "8                    901  [-443.5]     Oversampled\n",
       "9                     87   [370.5]    Undersampled\n",
       "10                   841  [-383.5]     Oversampled"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cvioqGRCxDoH"
   },
   "outputs": [],
   "source": [
    "# Define a condition\n",
    "condition = (agg_corpus['cluster_status']== 'Undersampled')\n",
    "\n",
    "# Use boolean indexing to get the DataFrame with rows that satisfy the condition\n",
    "filtered_df = agg_corpus[condition]\n",
    "\n",
    "# Get the index of the rows that satisfy the condition\n",
    "undersampled_clusters = filtered_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I_GSNs56xDoH",
    "outputId": "d403ec60-275a-4760-a62b-abeec6d535d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([4, 6, 9], dtype='int64', name='news_source_id')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "undersampled_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Scui5VMixDoH"
   },
   "outputs": [],
   "source": [
    "# Define a condition\n",
    "condition2 = (agg_corpus['cluster_status']== 'Oversampled')\n",
    "\n",
    "# Use boolean indexing to get the DataFrame with rows that satisfy the condition\n",
    "filtered_df2 = agg_corpus[condition2]\n",
    "\n",
    "# Get the index of the rows that satisfy the condition\n",
    "oversampled_clusters = filtered_df2.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b4FvY25CxDoH",
    "outputId": "75a64900-66c5-488a-9d12-0339da9ec19e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([5, 8, 10], dtype='int64', name='news_source_id')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oversampled_clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "77Dw6z5hxDoH"
   },
   "source": [
    "## Handling undersampled data clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UCJcj2YxDoI"
   },
   "outputs": [],
   "source": [
    "def resampling_undersample_clusters(df_minor):\n",
    "\n",
    "    # Upsampling minority class\n",
    "    df_minor_sample = resample(df_minor,\n",
    "\n",
    "                               # Upsample with replacement\n",
    "                               replace=True,\n",
    "\n",
    "                               # Number to match majority class\n",
    "                               n_samples= int(trimmed_mean),\n",
    "                               random_state=42)\n",
    "    return df_minor_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BA-mgHucxDoI"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(undersampled_clusters)):\n",
    "    df_minor = corpus_preprocessed_[corpus_preprocessed_.news_source_id == undersampled_clusters[i] ]\n",
    "    df_minor_sample = resampling_undersample_clusters(df_minor)\n",
    "\n",
    "    corpus_preprocessed_ = corpus_preprocessed_[corpus_preprocessed_.news_source_id != undersampled_clusters[i]]\n",
    "    corpus_preprocessed_ = pd.concat([corpus_preprocessed_ , df_minor_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "isZ0Q-JRxDoI"
   },
   "source": [
    "## Handling oversampled data clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uPCzgGmcxDoI"
   },
   "outputs": [],
   "source": [
    "def resampling_oversample_clusters(df_major):\n",
    "\n",
    "    # Upsampling minority class\n",
    "    df_major_sample = resample(df_major,\n",
    "\n",
    "                               # Oversample with replacement\n",
    "                               replace=True,\n",
    "\n",
    "                               # Number to match majority class\n",
    "                               n_samples=int(trimmed_mean),\n",
    "                               random_state=42)\n",
    "    return df_major_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38BODZtCxDoI"
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(oversampled_clusters)):\n",
    "    df_major = corpus_preprocessed_[corpus_preprocessed_.news_source_id == oversampled_clusters[i]]\n",
    "    df_major_sample = resampling_oversample_clusters(df_major)\n",
    "\n",
    "    corpus_preprocessed_ = corpus_preprocessed_[corpus_preprocessed_.news_source_id != oversampled_clusters[i]]\n",
    "    corpus_preprocessed_ = pd.concat([corpus_preprocessed_ , df_major_sample])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlDj51SmxDoI"
   },
   "source": [
    "Describe the resulting final dataset\n",
    "in terms of the total number of articles and the number of articles from each source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "469o26aYxDoI",
    "outputId": "042f651f-ca8c-4e50-ca28-d1a8cf29af7d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>news_source_id</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>457</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Articles\n",
       "news_source_id          \n",
       "1                    531\n",
       "2                    377\n",
       "3                    464\n",
       "4                    457\n",
       "5                    457\n",
       "6                    457\n",
       "7                    535\n",
       "8                    457\n",
       "9                    457\n",
       "10                   457"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_preprocessed_.groupby(['news_source_id']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rHszradFxDoI",
    "outputId": "15ceaea8-4db6-4369-f43a-5bfa0a9faf6d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>['india', 'say', 'continue', 'support', 'sri',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>['pope', 'francis', 'names', 'new', 'cardinals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>10</td>\n",
       "      <td>['government', 'race', 'time', 'finalise', 'de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4568</th>\n",
       "      <td>10</td>\n",
       "      <td>['mr', 'back', 'pm', 'morningth', 'july', 'e',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>10</td>\n",
       "      <td>['foreigner', 'sentence', 'death', 'drug', 'tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>10</td>\n",
       "      <td>['ntuc', 'union', 'give', 'week', 'ultimatum',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>10</td>\n",
       "      <td>['local', 'fuel', 'market', 'new', 'player', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4649 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles\n",
       "0                  2  ['cbsl', 'decide', 'renew', 'permit', 'money',...\n",
       "1                  2  ['xpress', 'pearl', 'disaster', 'sc', 'call', ...\n",
       "2                  2  ['torrential', 'rain', 'flood', 'kill', 'acros...\n",
       "3                  2  ['india', 'say', 'continue', 'support', 'sri',...\n",
       "4                  2  ['pope', 'francis', 'names', 'new', 'cardinals...\n",
       "...              ...                                                ...\n",
       "4249              10  ['government', 'race', 'time', 'finalise', 'de...\n",
       "4568              10  ['mr', 'back', 'pm', 'morningth', 'july', 'e',...\n",
       "3779              10  ['foreigner', 'sentence', 'death', 'drug', 'tr...\n",
       "4428              10  ['ntuc', 'union', 'give', 'week', 'ultimatum',...\n",
       "3763              10  ['local', 'fuel', 'market', 'new', 'player', '...\n",
       "\n",
       "[4649 rows x 2 columns]"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_preprocessed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wlTtBnf8xDoI"
   },
   "source": [
    "----------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2kE7B0DGxDoI"
   },
   "outputs": [],
   "source": [
    "corpus_normalized = corpus_preprocessed_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mOb8XocRxDoI"
   },
   "source": [
    "## PART B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIIahRtgxDoJ",
    "outputId": "1b8ed13e-6a6c-4b1d-d0aa-f4ce48a48a5d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUse one sparse and one dense vector representation for extracting features for training a classifier for this dataset.\\n\\nState and interpret the dimensions of the sparse matrix of the resulting dataset and justify the dimensions of the dense\\nmatrix of the resulting dataset respectively.\\n'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Use one sparse and one dense vector representation for extracting features for training a classifier for this dataset.\n",
    "\n",
    "State and interpret the dimensions of the sparse matrix of the resulting dataset and justify the dimensions of the dense\n",
    "matrix of the resulting dataset respectively.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IaYDRJVExDoJ"
   },
   "source": [
    "Use one sparse and one dense vector representation for extracting features for training a classifier for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTDm4nKixDoJ"
   },
   "source": [
    "## Extract features using sparse data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sCzdFNu6xDoJ"
   },
   "source": [
    "Term Frequency -->  means we summarize how often a given word appears within a document.<br>\n",
    "Inverse Document Frequency --> means figuring out the words that appear a lot across documents.<br>\n",
    "\n",
    "TF-IDF build a vocabulary of words which it has learned from the corpus data and it will assign a unique integer number to each of these words. Their will be maximum of 5000 unique words/features as we have set parameter max_features=5000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Qyxt3gLxDoJ",
    "outputId": "e628ff62-64df-4d1f-aead-536327f09912"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>['india', 'say', 'continue', 'support', 'sri',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>['pope', 'francis', 'names', 'new', 'cardinals...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>10</td>\n",
       "      <td>['government', 'race', 'time', 'finalise', 'de...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4568</th>\n",
       "      <td>10</td>\n",
       "      <td>['mr', 'back', 'pm', 'morningth', 'july', 'e',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>10</td>\n",
       "      <td>['foreigner', 'sentence', 'death', 'drug', 'tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>10</td>\n",
       "      <td>['ntuc', 'union', 'give', 'week', 'ultimatum',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>10</td>\n",
       "      <td>['local', 'fuel', 'market', 'new', 'player', '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4649 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles\n",
       "0                  2  ['cbsl', 'decide', 'renew', 'permit', 'money',...\n",
       "1                  2  ['xpress', 'pearl', 'disaster', 'sc', 'call', ...\n",
       "2                  2  ['torrential', 'rain', 'flood', 'kill', 'acros...\n",
       "3                  2  ['india', 'say', 'continue', 'support', 'sri',...\n",
       "4                  2  ['pope', 'francis', 'names', 'new', 'cardinals...\n",
       "...              ...                                                ...\n",
       "4249              10  ['government', 'race', 'time', 'finalise', 'de...\n",
       "4568              10  ['mr', 'back', 'pm', 'morningth', 'july', 'e',...\n",
       "3779              10  ['foreigner', 'sentence', 'death', 'drug', 'tr...\n",
       "4428              10  ['ntuc', 'union', 'give', 'week', 'ultimatum',...\n",
       "3763              10  ['local', 'fuel', 'market', 'new', 'player', '...\n",
       "\n",
       "[4649 rows x 2 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7_LOs_IExDoJ"
   },
   "source": [
    "### Feature : Weight of each term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3HWVEpOxDoJ"
   },
   "source": [
    "Term Frequency-Inverse Document Frequency (TF-IDF): Weighing the importance of words in a document relative to their frequency in the entire corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vKrl6OcqxDoJ"
   },
   "source": [
    "Initially it was number_of_tokens as features were (4649, 55857). But due to limited computation power, had to set a boundary for the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xa5lWvNaxDoJ"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "\n",
    "# Fit and transform the tokenized_content to create TF-IDF representation\n",
    "sparse_vector = tfidf_vectorizer.fit_transform(corpus_normalized['Articles'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i0q2o_5AxDoK",
    "outputId": "e67329f4-15ed-45b2-c03c-dc70e222fa3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 18161)\t0.025806700826348773\n",
      "  (0, 35432)\t0.02053093713350331\n",
      "  (0, 49119)\t0.024402895484684144\n",
      "  (0, 41638)\t0.03061448967695374\n",
      "  (0, 2288)\t0.03247754028530527\n",
      "  (0, 37043)\t0.03247754028530527\n",
      "  (0, 52521)\t0.03247754028530527\n",
      "  (0, 5592)\t0.03247754028530527\n",
      "  (0, 52436)\t0.03247754028530527\n",
      "  (0, 32343)\t0.03247754028530527\n",
      "  (0, 4717)\t0.03247754028530527\n",
      "  (0, 4716)\t0.03247754028530527\n",
      "  (0, 17817)\t0.04746125473254978\n",
      "  (0, 41902)\t0.062389491129382774\n",
      "  (0, 3747)\t0.04613680351664929\n",
      "  (0, 10624)\t0.06495508057061054\n",
      "  (0, 37826)\t0.048440255984750644\n",
      "  (0, 17695)\t0.054279845444213236\n",
      "  (0, 30116)\t0.06495508057061054\n",
      "  (0, 12467)\t0.04966642132791416\n",
      "  (0, 470)\t0.04190355205032465\n",
      "  (0, 47533)\t0.04761445123005975\n",
      "  (0, 50764)\t0.053391379629586\n",
      "  (0, 44236)\t0.03692217602173713\n",
      "  (0, 54161)\t0.06495508057061054\n",
      "  :\t:\n",
      "  (4648, 31142)\t0.13758355066292438\n",
      "  (4648, 25253)\t0.013987352290760178\n",
      "  (4648, 50764)\t0.10832023865656018\n",
      "  (4648, 44236)\t0.018726894426314252\n",
      "  (4648, 32277)\t0.023207396906492903\n",
      "  (4648, 52457)\t0.037941282474989534\n",
      "  (4648, 42191)\t0.02924351318510706\n",
      "  (4648, 17926)\t0.03884969562983307\n",
      "  (4648, 23996)\t0.04341646202085905\n",
      "  (4648, 51989)\t0.08468228644178062\n",
      "  (4648, 29182)\t0.1297431278589663\n",
      "  (4648, 8894)\t0.013325329027761115\n",
      "  (4648, 32258)\t0.08242863376102898\n",
      "  (4648, 28302)\t0.07043839396823845\n",
      "  (4648, 39095)\t0.08278981271223412\n",
      "  (4648, 9278)\t0.2156973993474408\n",
      "  (4648, 43750)\t0.04320610291091737\n",
      "  (4648, 17032)\t0.025729054352481254\n",
      "  (4648, 18053)\t0.02340054519309032\n",
      "  (4648, 25298)\t0.015737170933054764\n",
      "  (4648, 50048)\t0.030734819675915317\n",
      "  (4648, 40384)\t0.022029348121766698\n",
      "  (4648, 55336)\t0.016459930466402373\n",
      "  (4648, 26565)\t0.0329628604499887\n",
      "  (4648, 46793)\t0.0435977376545599\n"
     ]
    }
   ],
   "source": [
    "print(sparse_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p09EVNQbxDoK"
   },
   "source": [
    "\n",
    "\n",
    "tfidf_matrix is a sparse matrix where each row corresponds to a document, and each column corresponds to a unique word in the corpus. The entries in the matrix represent the TF-IDF score of each word in each document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2gW05GSyxDoK"
   },
   "source": [
    "Number of article , Uniuqe tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_qsKKgJqxDoK",
    "outputId": "ffa4da58-918b-4f11-b3c0-2244cb710a8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4649, 55857)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w0eCjE10xDoL",
    "outputId": "7933d632-4d87-43fa-dfcb-9bec1eb5d861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "feature_names_tfidf\n",
      "['aa' 'aaa' 'aad' ... 'zuraish' 'zurich' 'zwerner']\n"
     ]
    }
   ],
   "source": [
    "# Get the feature names (unique tokens)\n",
    "feature_names_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "print(\"feature_names_tfidf\")\n",
    "print(feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8BM6M09uxDoL",
    "outputId": "4949b681-caf1-4dbc-aa09-f816a3a146a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55857"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feature_names_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqGD2-QOxDoM"
   },
   "source": [
    "### Sparse vector representation - Test/Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xqkxFLiWxDoM"
   },
   "outputs": [],
   "source": [
    "#spliting the dataset\n",
    "sparse_train_X, sparse_test_X, sparse_train_Y, sparse_test_Y = model_selection.train_test_split(sparse_vector,corpus_normalized['news_source_id'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ172YDsxDoM"
   },
   "source": [
    "State and interpret the dimensions of the sparse matrix of the resulting dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6esLFEw5xDoM",
    "outputId": "3e927660-6a57-4bdd-c9c5-f473e188548e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3254, 55857)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_train_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5tMd_FSzxDoN",
    "outputId": "00bc7adb-3a79-46d9-cc92-c4983c5b31a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395, 55857)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_test_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2NqKn0wWxDoN",
    "outputId": "17ab61f5-aed6-4ac5-ec5b-431ffc89b6de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3254,)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_train_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M6JEoNbAxDoN",
    "outputId": "dbf96923-215f-4421-80e4-15ef512ba516"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1395,)\n"
     ]
    }
   ],
   "source": [
    "print(sparse_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2WjNX-03xDoN",
    "outputId": "63ddf478-6a88-4603-9150-fd0acdc57f76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 620)\t0.07196887367812128\n",
      "  (0, 40828)\t0.07196887367812128\n",
      "  (0, 43019)\t0.06863447963758372\n",
      "  (0, 20575)\t0.07196887367812128\n",
      "  (0, 28129)\t0.06626868941600733\n",
      "  (0, 20576)\t0.07196887367812128\n",
      "  (0, 32336)\t0.07196887367812128\n",
      "  (0, 44601)\t0.07196887367812128\n",
      "  (0, 52491)\t0.04969887739123259\n",
      "  (0, 30517)\t0.04969887739123259\n",
      "  (0, 1219)\t0.04969887739123259\n",
      "  (0, 1259)\t0.04969887739123259\n",
      "  (0, 12857)\t0.04969887739123259\n",
      "  (0, 1222)\t0.04942922610034495\n",
      "  (0, 1226)\t0.04969887739123259\n",
      "  (0, 24450)\t0.04969887739123259\n",
      "  (0, 1217)\t0.04969887739123259\n",
      "  (0, 22814)\t0.03175246377450839\n",
      "  (0, 16501)\t0.05026624907112643\n",
      "  (0, 35035)\t0.04969887739123259\n",
      "  (0, 11259)\t0.04969887739123259\n",
      "  (0, 35227)\t0.04969887739123259\n",
      "  (0, 8599)\t0.04969887739123259\n",
      "  (0, 32462)\t0.04969887739123259\n",
      "  (0, 48826)\t0.04969887739123259\n",
      "  :\t:\n",
      "  (3253, 34746)\t0.021194325292568858\n",
      "  (3253, 23996)\t0.015291433417543009\n",
      "  (3253, 36482)\t0.013838354000376194\n",
      "  (3253, 27779)\t0.026451831115369074\n",
      "  (3253, 8894)\t0.028159371645294258\n",
      "  (3253, 27784)\t0.02402847165848245\n",
      "  (3253, 38088)\t0.014336783487904936\n",
      "  (3253, 32258)\t0.02322532801959293\n",
      "  (3253, 9754)\t0.03682924238779842\n",
      "  (3253, 43750)\t0.04057958449501696\n",
      "  (3253, 18053)\t0.08241755824839701\n",
      "  (3253, 22918)\t0.013355153527206223\n",
      "  (3253, 25298)\t0.022170751857918312\n",
      "  (3253, 48779)\t0.010435284024838495\n",
      "  (3253, 12146)\t0.03922648138398057\n",
      "  (3253, 24453)\t0.028932664880486278\n",
      "  (3253, 55336)\t0.09275594337040342\n",
      "  (3253, 7754)\t0.021889932062415456\n",
      "  (3253, 7695)\t0.017630393523172163\n",
      "  (3253, 26565)\t0.11609637505253147\n",
      "  (3253, 46793)\t0.05374347770736589\n",
      "  (3253, 5813)\t0.016271325739598747\n",
      "  (3253, 30889)\t0.012651654158707924\n",
      "  (3253, 40750)\t0.015387427712656433\n",
      "  (3253, 12135)\t0.01585483536745521\n"
     ]
    }
   ],
   "source": [
    "print(sparse_train_X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxWg-rRuxDoN"
   },
   "source": [
    "## Extract features using dense data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here due to computation limitations, we have limited the number of features. By setting vector_size=1000, it will fetch best 1000 features by considering the vector value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5NufVa0xDoN"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "import nltk\n",
    "\n",
    "TOKENIZED_CORPUS = [nltk.word_tokenize(sentence)\n",
    "                    for sentence in corpus_normalized['Articles']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZjB6SQ8yxDoO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model parameters for the NN-based word2vec 'word embeddings':\n",
    "# size - dimension of the word vectors (tens to thousands)\n",
    "# window - window size to conside the context of a word\n",
    "# min_count - the minimum frequency of a word in the whole corpus to be included in vocabulary\n",
    "# sample - used to downsample the effects of the occurence of frequent words\n",
    "model = gensim.models.Word2Vec(TOKENIZED_CORPUS,\n",
    "                               vector_size=1000,\n",
    "                               window=8,\n",
    "                               min_count=1,\n",
    "                               workers = 4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rfgL1eTwxDoO"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Function to convert tokenized content into dense vectors\n",
    "def get_doc_embedding(doc_tokens):\n",
    "    embeddings = [model.wv[token] for token in doc_tokens if token in model.wv]\n",
    "    if embeddings:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    return np.zeros(model.vector_size)  # Return zeros for empty documents\n",
    "# Apply the function to the 'tokenized_content' column and store the dense vectors in a new column 'dense_vectors'\n",
    "corpus_normalized['dense_vectors'] = corpus_normalized['Articles'].apply(get_doc_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ucNmANOxDoO",
    "outputId": "48bef64c-1316-4fe7-87d1-66842f2408f3"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>dense_vectors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "      <td>[0.027175818, 0.47011563, -0.10083615, -0.1088...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "      <td>[0.0058154617, 0.48902646, -0.08228876, -0.095...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "      <td>[0.017496474, 0.48102412, -0.09090834, -0.1054...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>['india', 'say', 'continue', 'support', 'sri',...</td>\n",
       "      <td>[0.018921798, 0.48401517, -0.08358823, -0.1124...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>['pope', 'francis', 'names', 'new', 'cardinals...</td>\n",
       "      <td>[0.025925383, 0.46417668, -0.09185875, -0.1063...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4249</th>\n",
       "      <td>10</td>\n",
       "      <td>['government', 'race', 'time', 'finalise', 'de...</td>\n",
       "      <td>[0.027893167, 0.475432, -0.105379924, -0.11472...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4568</th>\n",
       "      <td>10</td>\n",
       "      <td>['mr', 'back', 'pm', 'morningth', 'july', 'e',...</td>\n",
       "      <td>[0.017281014, 0.4736882, -0.09014147, -0.10513...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3779</th>\n",
       "      <td>10</td>\n",
       "      <td>['foreigner', 'sentence', 'death', 'drug', 'tr...</td>\n",
       "      <td>[0.01417211, 0.4507376, -0.078855336, -0.08280...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4428</th>\n",
       "      <td>10</td>\n",
       "      <td>['ntuc', 'union', 'give', 'week', 'ultimatum',...</td>\n",
       "      <td>[0.024382606, 0.48901093, -0.07192123, -0.1036...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3763</th>\n",
       "      <td>10</td>\n",
       "      <td>['local', 'fuel', 'market', 'new', 'player', '...</td>\n",
       "      <td>[0.024819005, 0.48027125, -0.09387945, -0.1077...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4649 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles  \\\n",
       "0                  2  ['cbsl', 'decide', 'renew', 'permit', 'money',...   \n",
       "1                  2  ['xpress', 'pearl', 'disaster', 'sc', 'call', ...   \n",
       "2                  2  ['torrential', 'rain', 'flood', 'kill', 'acros...   \n",
       "3                  2  ['india', 'say', 'continue', 'support', 'sri',...   \n",
       "4                  2  ['pope', 'francis', 'names', 'new', 'cardinals...   \n",
       "...              ...                                                ...   \n",
       "4249              10  ['government', 'race', 'time', 'finalise', 'de...   \n",
       "4568              10  ['mr', 'back', 'pm', 'morningth', 'july', 'e',...   \n",
       "3779              10  ['foreigner', 'sentence', 'death', 'drug', 'tr...   \n",
       "4428              10  ['ntuc', 'union', 'give', 'week', 'ultimatum',...   \n",
       "3763              10  ['local', 'fuel', 'market', 'new', 'player', '...   \n",
       "\n",
       "                                          dense_vectors  \n",
       "0     [0.027175818, 0.47011563, -0.10083615, -0.1088...  \n",
       "1     [0.0058154617, 0.48902646, -0.08228876, -0.095...  \n",
       "2     [0.017496474, 0.48102412, -0.09090834, -0.1054...  \n",
       "3     [0.018921798, 0.48401517, -0.08358823, -0.1124...  \n",
       "4     [0.025925383, 0.46417668, -0.09185875, -0.1063...  \n",
       "...                                                 ...  \n",
       "4249  [0.027893167, 0.475432, -0.105379924, -0.11472...  \n",
       "4568  [0.017281014, 0.4736882, -0.09014147, -0.10513...  \n",
       "3779  [0.01417211, 0.4507376, -0.078855336, -0.08280...  \n",
       "4428  [0.024382606, 0.48901093, -0.07192123, -0.1036...  \n",
       "3763  [0.024819005, 0.48027125, -0.09387945, -0.1077...  \n",
       "\n",
       "[4649 rows x 3 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xXJWYFUCxDoP"
   },
   "outputs": [],
   "source": [
    "corpus_normalized.to_csv('dense_corpus.csv', index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sGYTH42CxDoP"
   },
   "source": [
    "### Dense vector representation - Test/Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s8rdKrIOxDoP"
   },
   "outputs": [],
   "source": [
    "dense_corpus = np.vstack(corpus_normalized['dense_vectors'].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Hj7qs4BxDoQ"
   },
   "outputs": [],
   "source": [
    "#spliting the dataset\n",
    "dense_train_X, dense_test_X, dense_train_Y, dense_test_Y = model_selection.train_test_split(dense_corpus,corpus_normalized['news_source_id'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k2If1tVzxDoQ",
    "outputId": "e0f690c5-e3b7-4a5c-a770-9f6b7955b5be",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3254, 1000), (1395, 1000), (3254,), (1395,))"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_train_X.shape, dense_test_X.shape, dense_train_Y.shape, dense_test_Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0x8b8Xi9xDoR",
    "outputId": "8002d046-03e8-477b-bba3-e8a03d303b3d",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01971179,  0.4839017 , -0.087449  , ...,  0.05809121,\n",
       "        -0.10273505,  0.3569534 ],\n",
       "       [ 0.00317502,  0.45791522, -0.08397418, ...,  0.07238246,\n",
       "        -0.09684703,  0.30982164],\n",
       "       [ 0.02487194,  0.46773273, -0.09541527, ...,  0.0614766 ,\n",
       "        -0.11313436,  0.3460021 ],\n",
       "       ...,\n",
       "       [ 0.02386056,  0.46107197, -0.0617226 , ...,  0.06179084,\n",
       "        -0.10115226,  0.32425943],\n",
       "       [ 0.00849587,  0.46239763, -0.08683304, ...,  0.06600481,\n",
       "        -0.09479102,  0.32984492],\n",
       "       [ 0.02263385,  0.465001  , -0.06930453, ...,  0.05425067,\n",
       "        -0.09862027,  0.33652294]], dtype=float32)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_test_X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BsHXdbHnxDoR"
   },
   "source": [
    "## PART C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q5DMaYodxDoR"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Train classifiers with the two (02) representations above using an explainable baseline algorithm,\n",
    "a more complex non-deep learning algorithm and a non-deep learning ensemble algorithm.\n",
    "\n",
    "Compare and contrast\n",
    "the performance of each of the classifiers and interpreting the results.\n",
    "\n",
    "Cluster Validation:\n",
    "If you have labeled data, perform cluster validation using metrics like silhouette score or Davies–Bouldin index\n",
    "to assess the quality of the clusters.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bStAlqeBxDoR"
   },
   "source": [
    "Train classifiers with the two (02) representations above using an explainable baseline algorithm,\n",
    "a more complex non-deep learning algorithm and a non-deep learning ensemble algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-EwM_EX9xDoR"
   },
   "source": [
    "(1). Explaninable baseline algorithm --> Logistic Regression <br>\n",
    "(2). Complex non-deep learning algorithm --> Random Forest Classiier <br>\n",
    "(3). Non-deep learning ensemble algorithm --> Support Vector Machine <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKfJ8fRhxDoR"
   },
   "source": [
    "### Sparse Data Representaion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA_sRHlcxDoS"
   },
   "source": [
    "Explaining the advantages in sparse data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYiSGpUfxDoS"
   },
   "source": [
    "@ sparse data representation, it saves memory compared to dense representation and we would only store non-zero values.<br>\n",
    "@ algorithms that take the advantage of sparsity like in linear regression algorithms, it reduce the complexity in sparse matrix operations.<br>\n",
    "@ speed computation, because mathematical operations involving zero values can be avoided.<br>\n",
    "@ improves model generalization by focusing on the most informative features, reducing the risk of overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zo3HOkFKxDoS"
   },
   "source": [
    "Explaining the disadvantages in sparse data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UkYbJz7xDoS"
   },
   "source": [
    "@  Will miss the relationships between features and their ordering.<br>\n",
    "@ sparse can become imbalanced, it can only effect for the algorithms that are sensitive for class distribution. <br>\n",
    "@ not suitable for small and moderate datasets. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1fANJ5ZMxDoS"
   },
   "source": [
    "(1). Explaninable baseline algorithm --> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vULzPa1xDoS",
    "outputId": "d502b628-92c8-4e43-dc22-1936ea02a979"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for LR\n",
      "Predicting for LR\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize classifiers\n",
    "logreg_clf = LogisticRegression()\n",
    "# Training the classifiers\n",
    "print(\"Training for LR\")\n",
    "logreg_clf.fit(sparse_train_X, sparse_train_Y)\n",
    "# Making predictions\n",
    "print(\"Predicting for LR\")\n",
    "y_pred_logreg = logreg_clf.predict(sparse_test_X)\n",
    "# Evaluating the classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0vCnrv92xDoS",
    "outputId": "78714fa3-cd9f-411f-da29-5f46c1f47508"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 0.996415770609319\n",
      "Logistic Regression Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       150\n",
      "           2       1.00      0.98      0.99       103\n",
      "           3       0.99      1.00      0.99       138\n",
      "           4       1.00      0.98      0.99       132\n",
      "           5       1.00      1.00      1.00       149\n",
      "           6       0.99      1.00      1.00       158\n",
      "           7       1.00      1.00      1.00       148\n",
      "           8       0.99      1.00      1.00       132\n",
      "           9       1.00      1.00      1.00       148\n",
      "          10       0.99      0.99      0.99       137\n",
      "\n",
      "    accuracy                           1.00      1395\n",
      "   macro avg       1.00      1.00      1.00      1395\n",
      "weighted avg       1.00      1.00      1.00      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy_logreg = accuracy_score(sparse_test_Y, y_pred_logreg)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy_logreg)\n",
    "print(\"Logistic Regression Classification Report:\")\n",
    "print(classification_report(sparse_test_Y, y_pred_logreg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WSzk6bFVxDoT"
   },
   "source": [
    "(2). Complex non-deep learning algorithm --> Random Forest Classiier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLkQ88omxDoT"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize classifiers\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2BgSGhjxDoT",
    "outputId": "c5f36c85-8184-4a4a-87bf-868347235da8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for RFC\n",
      "Predicting for RFC\n"
     ]
    }
   ],
   "source": [
    "print(\"Training for RFC\")\n",
    "rf_clf.fit(sparse_train_X, sparse_train_Y)\n",
    "# Making predictions\n",
    "print(\"Predicting for RFC\")\n",
    "y_pred_rf = rf_clf.predict(sparse_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3V0bFBQwxDoT",
    "outputId": "8310512b-9d29-4e13-9620-e9d56ce7fd76"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9971326164874552\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      1.00      1.00       150\n",
      "           2       1.00      0.98      0.99       103\n",
      "           3       0.98      1.00      0.99       138\n",
      "           4       1.00      0.99      1.00       132\n",
      "           5       1.00      1.00      1.00       149\n",
      "           6       0.99      1.00      1.00       158\n",
      "           7       1.00      1.00      1.00       148\n",
      "           8       1.00      1.00      1.00       132\n",
      "           9       1.00      1.00      1.00       148\n",
      "          10       1.00      0.99      1.00       137\n",
      "\n",
      "    accuracy                           1.00      1395\n",
      "   macro avg       1.00      1.00      1.00      1395\n",
      "weighted avg       1.00      1.00      1.00      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifiers\n",
    "accuracy_rf = accuracy_score(sparse_test_Y, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(sparse_test_Y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjWDuJCexDoT"
   },
   "source": [
    "(3). Non-deep learning ensemble algorithm --> Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8P07jQKjxDoT",
    "outputId": "1ec1aab5-1306-4c54-b49b-825ec1ae20ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for SVC\n",
      "Predicting for SVC\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize classifiers\n",
    "svm_clf = SVC()\n",
    "# Training the classifiers\n",
    "print(\"Training for SVC\")\n",
    "svm_clf.fit(sparse_train_X, sparse_train_Y)\n",
    "# Making predictions\n",
    "print(\"Predicting for SVC\")\n",
    "y_pred_svm = svm_clf.predict(sparse_test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oPm4uz4uxDoT",
    "outputId": "18a73c45-b6f0-4670-d0ef-9035738651b6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.996415770609319\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.99      1.00       150\n",
      "           2       1.00      0.98      0.99       103\n",
      "           3       0.99      1.00      0.99       138\n",
      "           4       1.00      0.99      1.00       132\n",
      "           5       1.00      1.00      1.00       149\n",
      "           6       1.00      1.00      1.00       158\n",
      "           7       1.00      1.00      1.00       148\n",
      "           8       0.98      1.00      0.99       132\n",
      "           9       1.00      1.00      1.00       148\n",
      "          10       1.00      0.99      1.00       137\n",
      "\n",
      "    accuracy                           1.00      1395\n",
      "   macro avg       1.00      1.00      1.00      1395\n",
      "weighted avg       1.00      1.00      1.00      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifiers\n",
    "accuracy_svm = accuracy_score(sparse_test_Y, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(sparse_test_Y, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qOOOFVrBxDoT"
   },
   "source": [
    "### Dense Data Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc-vzAfsxDoU"
   },
   "source": [
    "Explaining the advantages in sparse data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O10C8J0OxDoU"
   },
   "source": [
    "@ good for clustering algorithms that rely on distance metrics<br>\n",
    "@ easy to implement and understand <br>\n",
    "@ easy to debug <br>\n",
    "@ suitable for small datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7yHFvL11xDoU"
   },
   "source": [
    "Explaining the disadvantages in sparse data representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5KGSxiumxDoU"
   },
   "source": [
    "@ sensitive for the outlier data. <br>\n",
    "@ because of high dimensionality, it's difficult to interpret or find meaningful patterns. <br>\n",
    "@ it will take more computation time , therefore not suitable for real time applications or near real time applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "feRw_Tn_xDoU"
   },
   "source": [
    "(1). Explaninable baseline algorithm --> Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tkQ3dtVHxDoU",
    "outputId": "640562e5-faed-49f3-8891-b3f1d738a26a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hakim\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize the classifiers\n",
    "clf_logreg = LogisticRegression(max_iter=1000, random_state=42)\n",
    "# Train the classifiers\n",
    "clf_logreg.fit(dense_train_X, dense_train_Y)\n",
    "# Make predictions on the test set\n",
    "y_pred_logreg = clf_logreg.predict(dense_test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AQwBUKqMxDoU",
    "outputId": "011e0dbe-6c81-44fb-d50f-be86a8498a9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression): 0.7985663082437277\n",
      "\n",
      "Classification Report (Logistic Regression):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.69      0.95      0.80       153\n",
      "           2       0.75      0.79      0.77       127\n",
      "           3       0.83      0.70      0.76       132\n",
      "           4       0.86      0.90      0.88       156\n",
      "           5       0.89      0.86      0.88       144\n",
      "           6       0.92      0.75      0.83       134\n",
      "           7       0.89      0.94      0.91       142\n",
      "           8       0.84      0.78      0.81       139\n",
      "           9       0.73      0.89      0.80       120\n",
      "          10       0.59      0.43      0.50       148\n",
      "\n",
      "    accuracy                           0.80      1395\n",
      "   macro avg       0.80      0.80      0.79      1395\n",
      "weighted avg       0.80      0.80      0.79      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the classifiers\n",
    "accuracy_logreg = accuracy_score(dense_test_Y, y_pred_logreg)\n",
    "print(\"Accuracy (Logistic Regression):\", accuracy_logreg)\n",
    "print(\"\\nClassification Report (Logistic Regression):\\n\", classification_report(dense_test_Y, y_pred_logreg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QnPCfQhExDoU"
   },
   "source": [
    "(2). Complex non-deep learning algorithm --> Random Forest Classiier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7S3iRTYqxDoU"
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize classifiers\n",
    "rf_clf = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fXUTpmOExDoU",
    "outputId": "3e70f2a6-aec4-43dd-851e-7f10050b83a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for RFC\n",
      "Predicting for RFC\n"
     ]
    }
   ],
   "source": [
    "print(\"Training for RFC\")\n",
    "rf_clf.fit(dense_train_X, dense_train_Y)\n",
    "# Making predictions\n",
    "print(\"Predicting for RFC\")\n",
    "y_pred_rf = rf_clf.predict(dense_test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AA9Vq0TBxDoV",
    "outputId": "cb48c782-5848-4b9c-efd6-e637df78cb7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.9154121863799283\n",
      "Random Forest Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.94      0.95      0.95       153\n",
      "           2       0.83      0.86      0.84       127\n",
      "           3       0.89      0.89      0.89       132\n",
      "           4       0.94      0.97      0.96       156\n",
      "           5       0.95      0.96      0.95       144\n",
      "           6       0.99      1.00      0.99       134\n",
      "           7       0.99      0.93      0.96       142\n",
      "           8       0.93      0.90      0.91       139\n",
      "           9       0.96      1.00      0.98       120\n",
      "          10       0.74      0.70      0.72       148\n",
      "\n",
      "    accuracy                           0.92      1395\n",
      "   macro avg       0.91      0.92      0.92      1395\n",
      "weighted avg       0.91      0.92      0.91      1395\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifiers\n",
    "accuracy_rf = accuracy_score(dense_test_Y, y_pred_rf)\n",
    "print(\"Random Forest Accuracy:\", accuracy_rf)\n",
    "print(\"Random Forest Classification Report:\")\n",
    "print(classification_report(dense_test_Y, y_pred_rf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byTRX5pqxDoV"
   },
   "source": [
    "(3). Non-deep learning ensemble algorithm --> Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QMF749JxxDoV",
    "outputId": "3c69d3c3-125e-43f2-af03-cd5452a8e788"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for SVC\n",
      "Predicting for SVC\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "# Initialize classifiers\n",
    "svm_clf = SVC()\n",
    "# Training the classifiers\n",
    "print(\"Training for SVC\")\n",
    "svm_clf.fit(dense_train_X, dense_train_Y)\n",
    "# Making predictions\n",
    "print(\"Predicting for SVC\")\n",
    "y_pred_svm = svm_clf.predict(dense_test_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j6zUcXHExDoV",
    "outputId": "c8cda990-dc97-41c6-9492-ee5a8fc4159d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.36200716845878134\n",
      "SVM Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.18      0.99      0.31       153\n",
      "           2       0.00      0.00      0.00       127\n",
      "           3       0.86      0.29      0.43       132\n",
      "           4       0.42      0.15      0.22       156\n",
      "           5       0.67      0.21      0.32       144\n",
      "           6       0.94      0.65      0.77       134\n",
      "           7       0.46      0.87      0.60       142\n",
      "           8       0.00      0.00      0.00       139\n",
      "           9       0.76      0.31      0.44       120\n",
      "          10       0.79      0.10      0.18       148\n",
      "\n",
      "    accuracy                           0.36      1395\n",
      "   macro avg       0.51      0.36      0.33      1395\n",
      "weighted avg       0.50      0.36      0.32      1395\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hakim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hakim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\hakim\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the classifiers\n",
    "accuracy_svm = accuracy_score(dense_test_Y, y_pred_svm)\n",
    "print(\"SVM Accuracy:\", accuracy_svm)\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(dense_test_Y, y_pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Comparison\n",
    "\n",
    "\n",
    "model | sparse | dense \n",
    "| ----------- | ----------- | ----------- |\n",
    "Logistic Regrassion | 0.99 | 0.79\n",
    "Random forest | 0.99 | 0.915\n",
    "SVM | 0.99 | 0.36\n",
    "\n",
    " \n",
    " Compared to spare vector representation , reason for dense vector representation to have less accuracy is because of having more outlier articles based on the len of articles and because of high dimensionality, it's difficult to interpret or find meaningful patterns. With padding the sequences also cause the model to learn more zeros than the article content. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ibKQM5-_xDoV"
   },
   "source": [
    "# PART D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TLtjSCvpxDoV",
    "outputId": "e221374e-d768-4e72-ed1f-1e0dafe30cb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n(d) Train also three (03) deep learning classifiers with distinct architectures \\nusing two (02) embedding techniques and one (01) contextual embedding technique,\\njustifying the architectures you employ. Compare the performance of each of the models and interpret the results.\\n'"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "(d) Train also three (03) deep learning classifiers with distinct architectures\n",
    "using two (02) embedding techniques and one (01) contextual embedding technique,\n",
    "justifying the architectures you employ. Compare the performance of each of the models and interpret the results.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj2FkUpbxDoV"
   },
   "outputs": [],
   "source": [
    "def normalize_corpus_2(text):\n",
    "\n",
    "    text = expand_contractions(text, CONTRACTION_MAP)\n",
    "    text = lemmatize_text(text)\n",
    "    text = remove_special_characters(text)\n",
    "    text = remove_stopwords(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGppswRFxDoV",
    "outputId": "7a8b5b5d-8555-4a9a-e60e-01b0ad4d0264"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hakim\\AppData\\Local\\Temp\\ipykernel_2200\\3979527358.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  corpus['cleaned_article'] = corpus['english_context'].apply(lambda x : normalize_corpus_2(x))\n"
     ]
    }
   ],
   "source": [
    "corpus['cleaned_article'] = corpus['english_context'].apply(lambda x : normalize_corpus_2(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3FR686zxDoW",
    "outputId": "9ca95157-1015-4290-cca9-d2375817c7b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>non_english</th>\n",
       "      <th>english_context</th>\n",
       "      <th>word_count</th>\n",
       "      <th>preprocessed_article</th>\n",
       "      <th>cleaned_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "      <td>1510,20238:351510,202303:27()15()2023.,-2022.,...</td>\n",
       "      <td>CBSL decides not to renew permits of  money ch...</td>\n",
       "      <td>575</td>\n",
       "      <td>[cbsl, decide, renew, permit, money, changersj...</td>\n",
       "      <td>cbsl decide renew permit money changersjuly pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "      <td>-:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...</td>\n",
       "      <td>XPress Pearl disaster SC calls for report on c...</td>\n",
       "      <td>511</td>\n",
       "      <td>[xpress, pearl, disaster, sc, call, report, co...</td>\n",
       "      <td>xpress pearl disaster sc call report compensat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "      <td>||-10,20238:3510(09),10...|10,202311:2410(09),...</td>\n",
       "      <td>Sri Lanka News  Breaking News in Sri Lanka  Ad...</td>\n",
       "      <td>4048</td>\n",
       "      <td>[sri, lanka, news, breaking, news, sri, lanka,...</td>\n",
       "      <td>sri lanka news breaking news sri lanka adadera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "      <td>,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...</td>\n",
       "      <td>Torrential rain floods kill  across northern I...</td>\n",
       "      <td>553</td>\n",
       "      <td>[torrential, rain, flood, kill, across, northe...</td>\n",
       "      <td>torrential rain flood kill across northern ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>518</td>\n",
       "      <td>[driver, arrest, bus, accident, manampitiya, l...</td>\n",
       "      <td>driver arrest bus accident manampitiya leave d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8300</th>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>215</td>\n",
       "      <td>[special, commodity, levy, impose, import, egg...</td>\n",
       "      <td>special commodity levy impose import egg revis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8301</th>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "      <td>“”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...</td>\n",
       "      <td>This happened with full knowledge of President...</td>\n",
       "      <td>445</td>\n",
       "      <td>[happen, full, knowledge, president, harsha, m...</td>\n",
       "      <td>happen full knowledge president harsha morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8302</th>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>192</td>\n",
       "      <td>[new, lg, election, date, announce, march, mor...</td>\n",
       "      <td>new lg election date announce march morningth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8303</th>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>187</td>\n",
       "      <td>[hrcsl, look, prevailing, shortage, medicine, ...</td>\n",
       "      <td>hrcsl look prevailing shortage medicine mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8304</th>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "      <td>‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....</td>\n",
       "      <td>Prez calls BASL Prez a political lawyer  The M...</td>\n",
       "      <td>390</td>\n",
       "      <td>[prez, call, basl, prez, political, lawyer, mo...</td>\n",
       "      <td>prez call basl prez political lawyer morningth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7549 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      news_source_id                                           Articles  \\\n",
       "0                  2  CBSL decides not to renew permits of 15 money ...   \n",
       "1                  2  X-Press Pearl disaster: SC calls for report on...   \n",
       "2                  2  Sri Lanka News | Breaking News in Sri Lanka | ...   \n",
       "3                  2  Torrential rain, floods kill 22 across norther...   \n",
       "4                  2  Driver arrested over bus accident in Manampiti...   \n",
       "...              ...                                                ...   \n",
       "8300              10   Special commodity levy imposed on imported eg...   \n",
       "8301              10  “This happened with full knowledge of Presiden...   \n",
       "8302              10  New LG election dates to be announced on March...   \n",
       "8303              10  HRCSL to look into prevailing shortage of medi...   \n",
       "8304              10  Prez calls BASL Prez a ‘political’ lawyer | Th...   \n",
       "\n",
       "                                            non_english  \\\n",
       "0     1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
       "1     -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
       "2     ||-10,20238:3510(09),10...|10,202311:2410(09),...   \n",
       "3     ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
       "4     1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...   \n",
       "...                                                 ...   \n",
       "8300  |10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...   \n",
       "8301  “”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...   \n",
       "8302  03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...   \n",
       "8303  |10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...   \n",
       "8304  ‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....   \n",
       "\n",
       "                                        english_context  word_count  \\\n",
       "0     CBSL decides not to renew permits of  money ch...         575   \n",
       "1     XPress Pearl disaster SC calls for report on c...         511   \n",
       "2     Sri Lanka News  Breaking News in Sri Lanka  Ad...        4048   \n",
       "3     Torrential rain floods kill  across northern I...         553   \n",
       "4     Driver arrested over bus accident in Manampiti...         518   \n",
       "...                                                 ...         ...   \n",
       "8300   Special commodity levy imposed on imported eg...         215   \n",
       "8301  This happened with full knowledge of President...         445   \n",
       "8302  New LG election dates to be announced on March...         192   \n",
       "8303  HRCSL to look into prevailing shortage of medi...         187   \n",
       "8304  Prez calls BASL Prez a political lawyer  The M...         390   \n",
       "\n",
       "                                   preprocessed_article  \\\n",
       "0     [cbsl, decide, renew, permit, money, changersj...   \n",
       "1     [xpress, pearl, disaster, sc, call, report, co...   \n",
       "2     [sri, lanka, news, breaking, news, sri, lanka,...   \n",
       "3     [torrential, rain, flood, kill, across, northe...   \n",
       "4     [driver, arrest, bus, accident, manampitiya, l...   \n",
       "...                                                 ...   \n",
       "8300  [special, commodity, levy, impose, import, egg...   \n",
       "8301  [happen, full, knowledge, president, harsha, m...   \n",
       "8302  [new, lg, election, date, announce, march, mor...   \n",
       "8303  [hrcsl, look, prevailing, shortage, medicine, ...   \n",
       "8304  [prez, call, basl, prez, political, lawyer, mo...   \n",
       "\n",
       "                                        cleaned_article  \n",
       "0     cbsl decide renew permit money changersjuly pm...  \n",
       "1     xpress pearl disaster sc call report compensat...  \n",
       "2     sri lanka news breaking news sri lanka adadera...  \n",
       "3     torrential rain flood kill across northern ind...  \n",
       "4     driver arrest bus accident manampitiya leave d...  \n",
       "...                                                 ...  \n",
       "8300  special commodity levy impose import egg revis...  \n",
       "8301  happen full knowledge president harsha morning...  \n",
       "8302  new lg election date announce march morningth ...  \n",
       "8303  hrcsl look prevailing shortage medicine mornin...  \n",
       "8304  prez call basl prez political lawyer morningth...  \n",
       "\n",
       "[7549 rows x 7 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "PuCShKZDxDoW"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "JKUtI5TzxylJ"
   },
   "outputs": [],
   "source": [
    "corpus = pd.read_csv('deep_learning_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z-oABfuixDoY"
   },
   "source": [
    "#### 1. Transformer with GloVe Embeddings:\n",
    "Architecture:\n",
    "\n",
    "Input Layer: Tokenized sequences.<br>\n",
    "Embedding Layer: GloVe Embeddings for capturing global semantic relationships.<br>\n",
    "Transformer Encoder Layers: Self-attention mechanism for capturing long-range dependencies efficiently.<br>\n",
    "Global Average Pooling: Aggregates information from the entire sequence.<br>\n",
    "Dense Layers with Layer Normalization: Fully connected layers with layer normalization for better stability during training.<br>\n",
    "Output Layer: Final classification layer.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0OJQctM_xDoY"
   },
   "source": [
    "Justification:\n",
    "\n",
    "GloVe Embeddings: Effective for capturing global semantic relationships.<br>\n",
    "Transformer: Handles long-range dependencies efficiently, suitable for tasks where the context can be crucial regardless of the distance between words.<br>\n",
    "These architectures are just starting points, and you may need to fine-tune them based on the specific requirements of your task and dataset. Experimenting with different architectures, hyperparameters, and embeddings is crucial to finding the most effective model for your application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Haane8UPMa1q",
    "outputId": "8050d4c5-642f-471c-8045-55ddb062ed5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")\n",
    "\n",
    "corpus = pd.read_csv('/content/gdrive/My Drive/text_analytics/deep_learning_dataset_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 580
    },
    "id": "CLXLtBYpNpLS",
    "outputId": "12b6365d-a53d-4459-a00b-1aa9be39e1fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-6a3f6c9f-27ef-4b19-a863-48d61adb221b\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_source_id</th>\n",
       "      <th>Articles</th>\n",
       "      <th>non_english</th>\n",
       "      <th>english_context</th>\n",
       "      <th>word_count</th>\n",
       "      <th>preprocessed_article</th>\n",
       "      <th>cleaned_article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>CBSL decides not to renew permits of 15 money ...</td>\n",
       "      <td>1510,20238:351510,202303:27()15()2023.,-2022.,...</td>\n",
       "      <td>CBSL decides not to renew permits of  money ch...</td>\n",
       "      <td>575</td>\n",
       "      <td>['cbsl', 'decide', 'renew', 'permit', 'money',...</td>\n",
       "      <td>cbsl decide renew permit money changersjuly pm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>X-Press Pearl disaster: SC calls for report on...</td>\n",
       "      <td>-:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...</td>\n",
       "      <td>XPress Pearl disaster SC calls for report on c...</td>\n",
       "      <td>511</td>\n",
       "      <td>['xpress', 'pearl', 'disaster', 'sc', 'call', ...</td>\n",
       "      <td>xpress pearl disaster sc call report compensat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sri Lanka News | Breaking News in Sri Lanka | ...</td>\n",
       "      <td>||-10,20238:3510(09),10...|10,202311:2410(09),...</td>\n",
       "      <td>Sri Lanka News  Breaking News in Sri Lanka  Ad...</td>\n",
       "      <td>4048</td>\n",
       "      <td>['sri', 'lanka', 'news', 'breaking', 'news', '...</td>\n",
       "      <td>sri lanka news breaking news sri lanka adadera...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>Torrential rain, floods kill 22 across norther...</td>\n",
       "      <td>,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...</td>\n",
       "      <td>Torrential rain floods kill  across northern I...</td>\n",
       "      <td>553</td>\n",
       "      <td>['torrential', 'rain', 'flood', 'kill', 'acros...</td>\n",
       "      <td>torrential rain flood kill across northern ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...</td>\n",
       "      <td>Driver arrested over bus accident in Manampiti...</td>\n",
       "      <td>518</td>\n",
       "      <td>['driver', 'arrest', 'bus', 'accident', 'manam...</td>\n",
       "      <td>driver arrest bus accident manampitiya leave d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7544</th>\n",
       "      <td>10</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...</td>\n",
       "      <td>Special commodity levy imposed on imported eg...</td>\n",
       "      <td>215</td>\n",
       "      <td>['special', 'commodity', 'levy', 'impose', 'im...</td>\n",
       "      <td>special commodity levy impose import egg revis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7545</th>\n",
       "      <td>10</td>\n",
       "      <td>“This happened with full knowledge of Presiden...</td>\n",
       "      <td>“”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...</td>\n",
       "      <td>This happened with full knowledge of President...</td>\n",
       "      <td>445</td>\n",
       "      <td>['happen', 'full', 'knowledge', 'president', '...</td>\n",
       "      <td>happen full knowledge president harsha morning...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7546</th>\n",
       "      <td>10</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...</td>\n",
       "      <td>New LG election dates to be announced on March...</td>\n",
       "      <td>192</td>\n",
       "      <td>['new', 'lg', 'election', 'date', 'announce', ...</td>\n",
       "      <td>new lg election date announce march morningth ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7547</th>\n",
       "      <td>10</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>|10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...</td>\n",
       "      <td>HRCSL to look into prevailing shortage of medi...</td>\n",
       "      <td>187</td>\n",
       "      <td>['hrcsl', 'look', 'prevailing', 'shortage', 'm...</td>\n",
       "      <td>hrcsl look prevailing shortage medicine mornin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7548</th>\n",
       "      <td>10</td>\n",
       "      <td>Prez calls BASL Prez a ‘political’ lawyer | Th...</td>\n",
       "      <td>‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....</td>\n",
       "      <td>Prez calls BASL Prez a political lawyer  The M...</td>\n",
       "      <td>390</td>\n",
       "      <td>['prez', 'call', 'basl', 'prez', 'political', ...</td>\n",
       "      <td>prez call basl prez political lawyer morningth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7549 rows × 7 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6a3f6c9f-27ef-4b19-a863-48d61adb221b')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-6a3f6c9f-27ef-4b19-a863-48d61adb221b button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-6a3f6c9f-27ef-4b19-a863-48d61adb221b');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "<div id=\"df-8cafe9a9-c9ee-4659-b703-3b71a7a5b6fe\">\n",
       "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8cafe9a9-c9ee-4659-b703-3b71a7a5b6fe')\"\n",
       "            title=\"Suggest charts\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "     width=\"24px\">\n",
       "    <g>\n",
       "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
       "    </g>\n",
       "</svg>\n",
       "  </button>\n",
       "\n",
       "<style>\n",
       "  .colab-df-quickchart {\n",
       "      --bg-color: #E8F0FE;\n",
       "      --fill-color: #1967D2;\n",
       "      --hover-bg-color: #E2EBFA;\n",
       "      --hover-fill-color: #174EA6;\n",
       "      --disabled-fill-color: #AAA;\n",
       "      --disabled-bg-color: #DDD;\n",
       "  }\n",
       "\n",
       "  [theme=dark] .colab-df-quickchart {\n",
       "      --bg-color: #3B4455;\n",
       "      --fill-color: #D2E3FC;\n",
       "      --hover-bg-color: #434B5C;\n",
       "      --hover-fill-color: #FFFFFF;\n",
       "      --disabled-bg-color: #3B4455;\n",
       "      --disabled-fill-color: #666;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart {\n",
       "    background-color: var(--bg-color);\n",
       "    border: none;\n",
       "    border-radius: 50%;\n",
       "    cursor: pointer;\n",
       "    display: none;\n",
       "    fill: var(--fill-color);\n",
       "    height: 32px;\n",
       "    padding: 0;\n",
       "    width: 32px;\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart:hover {\n",
       "    background-color: var(--hover-bg-color);\n",
       "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "    fill: var(--button-hover-fill-color);\n",
       "  }\n",
       "\n",
       "  .colab-df-quickchart-complete:disabled,\n",
       "  .colab-df-quickchart-complete:disabled:hover {\n",
       "    background-color: var(--disabled-bg-color);\n",
       "    fill: var(--disabled-fill-color);\n",
       "    box-shadow: none;\n",
       "  }\n",
       "\n",
       "  .colab-df-spinner {\n",
       "    border: 2px solid var(--fill-color);\n",
       "    border-color: transparent;\n",
       "    border-bottom-color: var(--fill-color);\n",
       "    animation:\n",
       "      spin 1s steps(1) infinite;\n",
       "  }\n",
       "\n",
       "  @keyframes spin {\n",
       "    0% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "      border-left-color: var(--fill-color);\n",
       "    }\n",
       "    20% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    30% {\n",
       "      border-color: transparent;\n",
       "      border-left-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    40% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-top-color: var(--fill-color);\n",
       "    }\n",
       "    60% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "    }\n",
       "    80% {\n",
       "      border-color: transparent;\n",
       "      border-right-color: var(--fill-color);\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "    90% {\n",
       "      border-color: transparent;\n",
       "      border-bottom-color: var(--fill-color);\n",
       "    }\n",
       "  }\n",
       "</style>\n",
       "\n",
       "  <script>\n",
       "    async function quickchart(key) {\n",
       "      const quickchartButtonEl =\n",
       "        document.querySelector('#' + key + ' button');\n",
       "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
       "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
       "      try {\n",
       "        const charts = await google.colab.kernel.invokeFunction(\n",
       "            'suggestCharts', [key], {});\n",
       "      } catch (error) {\n",
       "        console.error('Error during call to suggestCharts:', error);\n",
       "      }\n",
       "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
       "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
       "    }\n",
       "    (() => {\n",
       "      let quickchartButtonEl =\n",
       "        document.querySelector('#df-8cafe9a9-c9ee-4659-b703-3b71a7a5b6fe button');\n",
       "      quickchartButtonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "    })();\n",
       "  </script>\n",
       "</div>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "      news_source_id                                           Articles  \\\n",
       "0                  2  CBSL decides not to renew permits of 15 money ...   \n",
       "1                  2  X-Press Pearl disaster: SC calls for report on...   \n",
       "2                  2  Sri Lanka News | Breaking News in Sri Lanka | ...   \n",
       "3                  2  Torrential rain, floods kill 22 across norther...   \n",
       "4                  2  Driver arrested over bus accident in Manampiti...   \n",
       "...              ...                                                ...   \n",
       "7544              10   Special commodity levy imposed on imported eg...   \n",
       "7545              10  “This happened with full knowledge of Presiden...   \n",
       "7546              10  New LG election dates to be announced on March...   \n",
       "7547              10  HRCSL to look into prevailing shortage of medi...   \n",
       "7548              10  Prez calls BASL Prez a ‘political’ lawyer | Th...   \n",
       "\n",
       "                                            non_english  \\\n",
       "0     1510,20238:351510,202303:27()15()2023.,-2022.,...   \n",
       "1     -:10,20238:36-:10,202303:18(10)-,,15.,,(10).-2...   \n",
       "2     ||-10,20238:3510(09),10...|10,202311:2410(09),...   \n",
       "3     ,2210,20238:36,2210,202302:2822,,.,.22,,,,.,.,...   \n",
       "4     1010,20238:361010,202301:58(09).,,,.,.,,.,10,4...   \n",
       "...                                                 ...   \n",
       "7544  |10,2023සිංහල|தமிழ்|252023|(27),.02().,02.,.,(...   \n",
       "7545  “”:|10,2023සිංහල|தமிழ்|“”:252023|()().,,,.(23)...   \n",
       "7546  03|10,2023සිංහල|தமிழ்|03242023|202303,2023,.,0...   \n",
       "7547  |10,2023සිංහල|தமிழ்|242023|(),.,.’,.,-()..,,.(...   \n",
       "7548  ‘’|10,2023සිංහල|தமிழ்|‘’232023|’,(),’()()()“”....   \n",
       "\n",
       "                                        english_context  word_count  \\\n",
       "0     CBSL decides not to renew permits of  money ch...         575   \n",
       "1     XPress Pearl disaster SC calls for report on c...         511   \n",
       "2     Sri Lanka News  Breaking News in Sri Lanka  Ad...        4048   \n",
       "3     Torrential rain floods kill  across northern I...         553   \n",
       "4     Driver arrested over bus accident in Manampiti...         518   \n",
       "...                                                 ...         ...   \n",
       "7544   Special commodity levy imposed on imported eg...         215   \n",
       "7545  This happened with full knowledge of President...         445   \n",
       "7546  New LG election dates to be announced on March...         192   \n",
       "7547  HRCSL to look into prevailing shortage of medi...         187   \n",
       "7548  Prez calls BASL Prez a political lawyer  The M...         390   \n",
       "\n",
       "                                   preprocessed_article  \\\n",
       "0     ['cbsl', 'decide', 'renew', 'permit', 'money',...   \n",
       "1     ['xpress', 'pearl', 'disaster', 'sc', 'call', ...   \n",
       "2     ['sri', 'lanka', 'news', 'breaking', 'news', '...   \n",
       "3     ['torrential', 'rain', 'flood', 'kill', 'acros...   \n",
       "4     ['driver', 'arrest', 'bus', 'accident', 'manam...   \n",
       "...                                                 ...   \n",
       "7544  ['special', 'commodity', 'levy', 'impose', 'im...   \n",
       "7545  ['happen', 'full', 'knowledge', 'president', '...   \n",
       "7546  ['new', 'lg', 'election', 'date', 'announce', ...   \n",
       "7547  ['hrcsl', 'look', 'prevailing', 'shortage', 'm...   \n",
       "7548  ['prez', 'call', 'basl', 'prez', 'political', ...   \n",
       "\n",
       "                                        cleaned_article  \n",
       "0     cbsl decide renew permit money changersjuly pm...  \n",
       "1     xpress pearl disaster sc call report compensat...  \n",
       "2     sri lanka news breaking news sri lanka adadera...  \n",
       "3     torrential rain flood kill across northern ind...  \n",
       "4     driver arrest bus accident manampitiya leave d...  \n",
       "...                                                 ...  \n",
       "7544  special commodity levy impose import egg revis...  \n",
       "7545  happen full knowledge president harsha morning...  \n",
       "7546  new lg election date announce march morningth ...  \n",
       "7547  hrcsl look prevailing shortage medicine mornin...  \n",
       "7548  prez call basl prez political lawyer morningth...  \n",
       "\n",
       "[7549 rows x 7 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "id": "MnjSSm7RPBtg"
   },
   "outputs": [],
   "source": [
    "# Convert the string to a list of integers\n",
    "import ast\n",
    "corpus['preprocessed_article_']  = corpus['preprocessed_article'] .apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "id": "jN6tAsxiMVpL"
   },
   "outputs": [],
   "source": [
    "# Create a vocabulary mapping words to indices\n",
    "vocab = {}\n",
    "index = 0\n",
    "\n",
    "for article_tokens in corpus['preprocessed_article_'].values:\n",
    "    for token in article_tokens:\n",
    "      if token not in vocab:\n",
    "            vocab[token] = index\n",
    "            index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "id": "sbTXa13nMW7-"
   },
   "outputs": [],
   "source": [
    "# Convert token lists to word indices using the vocabulary\n",
    "corpus['word_indices'] = corpus['preprocessed_article_'].apply(lambda x : [vocab[token] for token in x ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "Pf0312McRcI_"
   },
   "outputs": [],
   "source": [
    "#spliting the dataset\n",
    "dense_train_X, dense_test_X, dense_train_Y, dense_test_Y = model_selection.train_test_split(corpus['word_indices'],corpus['news_source_id'],test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "zs2op0UfovyX"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "# Assuming you have a dataset class, adjust this accordingly\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Implement based on your dataset structure\n",
    "        article = self.data[idx]\n",
    "        target = self.targets[idx]\n",
    "\n",
    "        # Pad or trim the article to a fixed length (adjust as needed)\n",
    "        max_length = 100\n",
    "        padded_article = article[:max_length] + [0] * (max_length - len(article))\n",
    "\n",
    "        return torch.tensor(padded_article), torch.tensor(target)\n",
    "\n",
    "# Simple Transformer model for clustering\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, num_heads, num_layers):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(input_dim, embed_dim)\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            num_encoder_layers=num_layers,\n",
    "            num_decoder_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(embed_dim, 1)\n",
    "\n",
    "    def forward(self, x, offsets):\n",
    "        x = self.embedding(x, offsets=offsets)\n",
    "        x = self.transformer(x, x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "9Sa1B1okqL3T"
   },
   "outputs": [],
   "source": [
    "# Load GloVe embeddings\n",
    "glove = GloVe(name='6B', dim =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "i12D9lNDxLPs"
   },
   "outputs": [],
   "source": [
    "# Assuming you have features and targets as lists\n",
    "article_features = dense_train_X.tolist()   # Replace with your actual feature data\n",
    "article_targets = dense_train_Y.tolist()   # Replace with your actual target data\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "article_dataset = ArticleDataset(data=article_features, targets=article_targets)\n",
    "\n",
    "# You can now use a DataLoader to efficiently iterate over batches\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(article_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lhLrVhmZxNrA",
    "outputId": "f01811d0-e0cd-4e04-921e-71d88d0ec596"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:282: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "input_dim = len(glove.stoi)\n",
    "embed_dim = 100  # Adjust based on your GloVe embedding dimension\n",
    "num_heads = 4\n",
    "num_layers = 2\n",
    "model = TransformerModel(input_dim, embed_dim, num_heads, num_layers)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gZ__g7TWxQKG",
    "outputId": "190c73e3-26b3-4a5c-87c0-1c08f130abc9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 3.551928758621216\n",
      "Epoch [2/10], Loss: 3.5381460189819336\n",
      "Epoch [3/10], Loss: 3.5182275772094727\n",
      "Epoch [4/10], Loss: 3.71205472946167\n",
      "Epoch [5/10], Loss: 3.6984167098999023\n",
      "Epoch [6/10], Loss: 3.5422472953796387\n",
      "Epoch [7/10], Loss: 3.826603651046753\n",
      "Epoch [8/10], Loss: 4.0460357666015625\n",
      "Epoch [9/10], Loss: 2.966336250305176\n",
      "Epoch [10/10], Loss: 4.84248161315918\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_loader:\n",
    "        features, targets = batch\n",
    "\n",
    "        # Assuming features are indices and need to be converted to embeddings\n",
    "        feature_embeddings = glove[torch.tensor(features, dtype=torch.long)]\n",
    "\n",
    "        # Calculate offsets based on the lengths of the sequences in the batch\n",
    "        offsets = torch.tensor([0] + [len(seq) for seq in features], dtype=torch.long).cumsum(dim=0)[:-1]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Pass features and offsets to the model\n",
    "        outputs = model(feature_embeddings, offsets)\n",
    "        loss = criterion(outputs.squeeze(), targets)  # Assuming targets are 1D tensor\n",
    "\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "id": "zIo-DCYKxVDb"
   },
   "outputs": [],
   "source": [
    "# Assuming you have features and targets as lists\n",
    "article_features_test = dense_test_X.tolist()   # Replace with your actual feature data\n",
    "article_targets_test= dense_test_Y.tolist()   # Replace with your actual target data\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "val_dataset = ArticleDataset(data=article_features_test, targets=article_targets_test)\n",
    "\n",
    "# Assuming you have a separate DataLoader for validation or test set\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "total_correct = 0\n",
    "total_samples = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KyOZnAfAxn9u",
    "outputId": "25963799-9d06-4b86-eea5-419973b9f972"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 68.40%\n"
     ]
    }
   ],
   "source": [
    "# No need to compute gradients during evaluation\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        features, targets = batch\n",
    "\n",
    "        # Assuming you have offsets (lengths of the sequences) for each article\n",
    "        # Calculate offsets based on the lengths of the sequences\n",
    "        offsets = torch.tensor([0] + [len(seq) for seq in features], dtype=torch.long).cumsum(dim=0)[:-1]\n",
    "\n",
    "        # Pass features and offsets to the model\n",
    "        outputs = model(features, offsets)\n",
    "\n",
    "        # Get predicted labels\n",
    "        _, predicted_labels = torch.max(outputs, 1)\n",
    "\n",
    "        # Update total_correct and total_samples\n",
    "        total_correct += (predicted_labels == targets).sum().item()\n",
    "        total_samples += len(targets)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = total_correct / total_samples\n",
    "\n",
    "print(f'Validation Accuracy: {accuracy * 100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
